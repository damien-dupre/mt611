---
title: "MT611 - Quantitative Research Methods"
subtitle: "Lecture 4: Hypotheses with Categorical Variables"
author: "Damien Dupr√©"
date: "Dublin City University"
output:
  xaringan::moon_reader:
    css: ["default", "metropolis", "metropolis-fonts", "css/custom_design.css"]
    lib_dir: libs
    nature:
      beforeInit: "libs/cols_macro.js"
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
# general options --------------------------------------------------------------
options(scipen = 999)
set.seed(123)
# chunk options ----------------------------------------------------------------
knitr::opts_chunk$set(
  cache.extra = knitr::rand_seed, 
  message = FALSE, 
  warning = FALSE, 
  error = FALSE, 
  echo = FALSE,
  cache = FALSE,
  comment = "", 
  fig.align = "center", 
  fig.retina = 3
  )
# libraries --------------------------------------------------------------------
library(tidyverse)
library(fontawesome)
library(DiagrammeR)
library(patchwork)
library(ggrepel)
library(papaja)
library(knitr)
library(kableExtra)

# data -------------------------------------------------------------------------
dnd <- readr::read_csv(here::here("data/dnd.csv"))  

  # tibble::tibble(
  #   gender = sample(c("male", "female"), 20, replace = TRUE),
  #   location = sample(c("Ireland", "France", "Australia"), 20, replace = TRUE),
  #   perf = rnorm(20, mean = 4, sd = 2),
  #   salary = rnorm(20, mean = 30000, sd = 1000),
  #   js_score = -55 + 0.002 * salary + rnorm(20, mean = 2, sd = 1)
  # ) %>%
  # tibble::rownames_to_column("employee") %>%
  # dplyr::mutate(
  #   js_score = case_when(
  #     js_score > 10 ~ 10,
  #     js_score < 0 ~ 0,
  #     TRUE ~ js_score
  #   ),
  #   perf = case_when(
  #     perf > 10 ~ 10,
  #     perf < 0 ~ 0,
  #     TRUE ~ perf
  #   ),
  #   salary_c = case_when(
  #     salary >= mean(salary) ~ "high",
  #     salary < mean(salary) ~ "low"
  #   ),
  #   perf_c = case_when(
  #     perf >= mean(perf) + sd(perf) ~ "high",
  #     perf < mean(perf) + sd(perf) & perf >= mean(perf) - sd(perf) ~ "medium",
  #     perf < mean(perf) - sd(perf) ~ "low"
  #   ),
  # ) %>%
  # readr::write_csv(here::here("data/dnd.csv"))

# analyses ---------------------------------------------------------------------
m_js_high <- mean(dnd$js_score[dnd$salary_c == "high"])
m_js_low <- mean(dnd$js_score[dnd$salary_c == "low"])
lm_1 <- lm(js_score ~ salary, data = dnd) %>% apa_print
lm_2 <- lm(js_score ~ salary*perf, data = dnd) %>% apa_print
lm_c <- lm(js_score ~ salary_c, data = dnd) %>% apa_print
lm_c2 <- dnd %>% 
  dplyr::mutate(salary_c = factor(salary_c, level = c("low", "high"))) %>% 
  lm(js_score ~ salary_c, data = .) %>% apa_print
lm_c3 <- lm(js_score ~ location, data = dnd) %>% aov %>% apa_print

```

class: inverse, mline, center, middle

# 1. Hypotheses with Categorical Predictors having 2 Modalities

---

# $t$-test vs. Linear Regression

Why $t$-test and Linear Regression obtain the exact **same** results?

.pull-left[

Hypothesis Testing in $t$-test

- H1: $\mu_{1} \neq \mu_{2}$
- H0: $\mu_{1} = \mu_{2}$
]

.pull-right[

Hypothesis Testing in Linear Regression

- H1: $\beta_{1} \neq 0$
- H0: $\beta_{1} = 0$

]

If $\mu_{1} \neq \mu_{2}$ then the slope of the line between group averages is different than 0 ( $\beta_{1} \neq 0$ ).

If $\mu_{1} = \mu_{2}$ then the slope of the line between group averages is equal to 0 ( $\beta_{1} = 0$ ).

What you don't know is that the Linear Regression automatically recode all Categorical variables to Continuous variables.

---

# Example of Categorical Coding

.pull-left[
Imagine we decide to sample 3 males and 3 females from the classroom and we want to see if the difference between their math exams averages is due **to sampling luck or is reflecting a real difference in the population**.

That is, **is the difference between male and female student statistically significant?**
]

.pull-right[
```{r}
df2 <- 
  data.frame(
    participant = c("ppt1", "ppt2", "ppt3", "ppt4", "ppt5", "ppt6"), 
    gender = c("Female", "Male", "Male", "Female", "Male", "Female"),
    math_results = c(89, 64, 72, 77, 78, 69)
  ) %>% 
  dplyr::mutate(
    gender_c = case_when(gender == "Female" ~ 1, gender == "Male" ~ 2)
  )

df2 %>% 
  dplyr::select(-gender_c) %>% 
  knitr::kable(format = "html")
```
]

---

# Example of Categorical Coding

In fact, **comparing female vs. male** is the same as **comparing female coded 1 and male coded 2**

.pull-left[
```{r fig.height=5}
df2 %>% 
  ggplot(aes(x = gender, y = math_results)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  theme_bw() +
  theme(
    text = element_text(size = 14)
  )
```
]

.pull-right[
```{r fig.height=5}
df2 %>% 
  ggplot(aes(x = gender_c, y = math_results)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  scale_x_continuous(breaks = c(1, 2), limits = c(0.5, 2.5)) +
  theme_bw() +
  theme(
    text = element_text(size = 14)
  )
```
]

---

# Categorical Coding in Linear Regression

For Categorical variables having 2 modalities, **by default the Linear Regression recodes with the alphabetical order** (Female first then Male).

However you can manually choose to recode the variable yourself by creating a new variable, such as...

.pull-left[

In JAMOVI, recode variable gender as 2 if is "female" else as 1

`IF(gender == "female", 2, 1)`
]

.pull-right[

In JAMOVI, recode variable gender as 1 if is "female" else as 3

`IF(gender == "female", 1, 3)`
]

.center[**What happens with these different codings? How different the results are?**]

--

.pull-left[
If the order of codes is reversed:

- Sign of the effect estimate $\beta$ changes but not its size
- Test of the intercept is different (value of Y when X = 0 changes)
]

.pull-right[
If the distance between codes increases:

- Size of the effect estimate $\beta$ changes but not its sign
- Test of the intercept is different (value of Y when X = 0 changes)
]

---

# Categorical Coding in Linear Regression

.pull-left[

Female = 1 and Male = 2

```{r fig.height=4}
df2 %>% 
  ggplot(aes(x = gender_c, y = math_results)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  scale_x_continuous(breaks = c(1, 2), limits = c(0.5, 2.5)) +
  theme_bw() +
  theme(
    text = element_text(size = 14)
  )
```

```{r results='asis'}
df2 %>% 
  lm(data = ., formula = math_results ~ gender_c) %>% 
  broom::tidy() %>% 
  dplyr::mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) %>% 
  knitr::kable(digits = 2) %>%
  kable_styling(font_size = 12)
```
]

.pull-right[

Female = 2 and Male = 1

```{r fig.height=4}
df2  %>% 
  dplyr::mutate(gender_c = case_when(
    gender == "Female" ~ 2,
    gender == "Male" ~ 1
  )) %>% 
  ggplot(aes(x = gender_c, y = math_results)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  scale_x_continuous(breaks = c(1, 2), limits = c(0.5, 2.5)) +
  theme_bw() +
  theme(
    text = element_text(size = 14)
  )
```

```{r results='asis'}
df2 %>% 
  dplyr::mutate(gender_c = case_when(
    gender == "Female" ~ 2,
    gender == "Male" ~ 1
  )) %>% 
  lm(data = ., formula = math_results ~ gender_c) %>% 
  broom::tidy() %>% 
  dplyr::mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) %>% 
  knitr::kable(digits = 2) %>%
  kable_styling(font_size = 12)
```
]

---

# Categorical Coding in Linear Regression

.pull-left[

Female = 1 and Male = 2

```{r fig.height=4}
df2 %>% 
  ggplot(aes(x = gender_c, y = math_results)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  scale_x_continuous(breaks = c(1, 2), limits = c(0.5, 2.5)) +
  theme_bw() +
  theme(
    text = element_text(size = 14)
  )
```

```{r results='asis'}
df2 %>% 
  lm(data = ., formula = math_results ~ gender_c) %>% 
  broom::tidy() %>% 
  dplyr::mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) %>% 
  knitr::kable(digits = 2) %>%
  kable_styling(font_size = 12)
```
]

.pull-right[

Female = 1 and Male = 3

```{r fig.height=4}
df2  %>% 
  dplyr::mutate(gender_c = case_when(
    gender == "Female" ~ 1,
    gender == "Male" ~ 3
  )) %>% 
  ggplot(aes(x = gender_c, y = math_results)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  scale_x_continuous(breaks = c(1, 3), limits = c(0.5, 3.5)) +
  theme_bw() +
  theme(
    text = element_text(size = 14)
  )
```

```{r results='asis'}
df2 %>% 
  dplyr::mutate(gender_c = case_when(
    gender == "Female" ~ 1,
    gender == "Male" ~ 3
  )) %>% 
  lm(data = ., formula = math_results ~ gender_c) %>% 
  broom::tidy() %>% 
  dplyr::mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) %>% 
  knitr::kable(digits = 2) %>%
  kable_styling(font_size = 12)
```
]

---

# Categorical Predictor with 2 Modalities

Let's use another example with the `dnd` dataset!

### Variable transformation

Instead of using $salary$ as a **continuous variable**, let's convert it as $salary\_c$ which is a **categorical variable**:
- Everything higher than or equal to salary average is labelled "**high**" salary
- Everything lower than salary average is labelled "**low**" salary

### Hypothesis

The average $js\_score$ of employer having a **high** $salary\_c$ is higher than for those having a **low** $salary\_c$

### In mathematical terms

$$H_a: \mu(js\_score)_{high\,salary} > \mu(js\_score)_{low\,salary}$$
$$H_0: \mu(js\_score)_{high\,salary} = \mu(js\_score)_{low\,salary}$$

---

# Categorical Predictor with 2 Modalities

### The trick to remember

**Comparing the difference between two averages is the same as comparing the slope of the line crossing these two averages**

### Explanation

.pull-left[
- If two averages are **equal**, then the **slope of the line crossing these two averages is 0**
- If two averages are **not equal**, then **the slope of the line crossing these two averages is not 0**
]

.pull-right[
```{r fig.width=4, fig.height=4}
ggplot(dnd, aes(x = salary_c, y = js_score)) + 
  geom_jitter(width = 0.1) +
  geom_segment(x = 1, xend = 2, y = m_js_high, yend = m_js_low, lwd = 2, color = "red") +
  geom_hline(yintercept = (m_js_high + m_js_low)/2, linetype = "dashed") +
  stat_summary(fun = mean, geom = "errorbar", aes(ymax = ..y.., ymin = ..y..), lwd = 2, color = "blue") +
  theme(
    legend.position = "none",
    text = element_text(size = 20)
    ) +
  labs(caption = "high coded 1 and low coded 2 (default)")
```
]

---

# Categorical Predictor with 2 Modalities

### Warning

JAMOVI and other software **automatically code categorical variable following alphabetical order** but sometimes you need your own system of coding to make. 

For example, here **low coded with the value 1** and **high coded with the value 2** would make more sense

```{r fig.width=4, fig.height=4}
dnd %>% 
  dplyr::mutate(salary_c = factor(salary_c, levels = c("low", "high"))) %>% 
  ggplot(aes(x = salary_c, y = js_score)) + 
  geom_jitter(width = 0.1) +
  geom_segment(x = 1, xend = 2, yend = m_js_high, y = m_js_low, lwd = 2, color = "red") +
  geom_hline(yintercept = (m_js_high + m_js_low)/2, linetype = "dashed") +
  stat_summary(fun = mean, geom = "errorbar", aes(ymax = ..y.., ymin = ..y..), lwd = 2, color = "blue") +
  theme(
    legend.position = "none",
    text = element_text(size = 20)
    )
```

---

# Categorical Predictor with 2 Modalities

The way how categorical variables are coded will influence:
- The sign of the estimate (positive vs. negative)
- The value of the non-standardized estimate

But **it doesn't change the value of the statistical test** nor the $p$-value obtained

Note: This test is usually done using a $t$-test but will produce the same result as $t$-test is a special case of the General Linear Model

### To sum up

**To test the influence of a categorical predictor** variable either nominal or ordinal **having two modalities** (e.g., high vs. low, male vs. female, France vs. Ireland), it is possible to **test if the $\beta$ associated to this predictor is significantly different, higher or lower than 0**.

### Equation

$$js\_score = \beta_{0} + \beta_{1} * salary\_c + \epsilon$$

---

# Testing Categorical Predictors

### In JAMOVI

1. Open your file
2. Set variables in their **correct type** (continuous, cat. nominal or cat. ordinal)
3. **Analyses > Regression > Linear Regression**
4. Set $js\_score$ as DV and $salary\_c$ as Factors

```{r out.width = "100%"}
knitr::include_graphics("img/jamovi_lm_main_c2.png")
```

---

# Testing Categorical Predictors

### Model

> The prediction provided by the model with all predictors is significantly better than a model without predictors (** `r lm_c$full_result$modelfit$r2`**).

### Hypothesis (high = 1 vs. low = 2, default)

> The effect of $salary\_c$ on $js\_score$ is statistically significant, therefore $H_{0}$ can be rejected (** `r lm_c$full_result$salary_c`**).

### Hypothesis (low = 1 vs. high = 2, recoded)

> The effect of $salary\_c$ on $js\_score$ is statistically significant, therefore $H_{0}$ can be rejected (** `r lm_c2$full_result$salary_c`**).

---

# Coding of Categorical Predictors

Choosing 1 and 2 are **just arbitrary numerical values** but any other possibility will produce the same $p$-value

However choosing codes separated by 1 is handy because it's easily interpretable, the **non-standardized estimate corresponds to the change from one modality to another**:

> When "low" is coded 1 and "high" coded 2, the average $js\_score$ for "high" $salary\_c$ employes is `r round(m_js_high - m_js_low, 2)` higher than the average $js\_score$ for "low" $salary\_c$ employees

---

# Coding of Categorical Predictors

### Special case called **Dummy Coding** when a modality is coded 0 and the other 1:
- Then the intercept, value of $js\_score$ when salary is 0 corresponds to the modality coded 0
- The test of the intercept is the test of the average for the modality coded 0 against an average of 0
- Is called simple effect

### Special case called **Deviation Coding** when a modality is coded 1 and the other -1:
- Then the intercept, corresponds to the average between the two modalities
- The test of the intercept is the test of the average for the variable
- However, the distance between 1 and -1 is 2 units so the estimate is not as easy to interpret, therefore it is possible to choose modalities coded 0.5 vs. -0.5 instead

---

# Interaction with Categorical Variables

### In JAMOVI

1. Open your file
2. Set variables according their type
3. **Analyses > Regression > Linear Regression**
4. Set $js\_score$ as DV and $salary\_c$ as well as $gender$ as Factors
4. In **Model Builder** option: select both $salary\_c$ and $gender$ to bring them in the Factors at once

### Model Tested

$$js\_score = \beta_{0} + \beta_{1}.salary\_c + \beta_{2}.gender + \beta_{3}.salary\_c*gender + \epsilon$$

Note: The test of the interaction effect corresponds to the test of a variable resulting from the multiplication between the codes of $salary\_c$ and the codes of $gender$.

---

# Interaction with Categorical Variables

```{r out.width = "100%"}
knitr::include_graphics("img/jamovi_lm_main_cint.png")
```


---

class: inverse, mline, center, middle

# 2. Hypotheses with Categorical Predictor having 3+ Modalities

---

# Categorical Predictor with 3+ Modalities

### Problem with more than 2 groups

I would like to test the effect of the variable $location$ which has 3 modalities: "Ireland", "France" and "Australia".

```{r}
knitr::include_graphics("img/jamovi_lm_main_c31.png")
```

There is not 1 result for $location$ but 2!
- Comparison of "France" vs. "Australia"
- Comparison of "Ireland" vs. "Australia"

### How can I test the effect of the full variable?

---

# Categorical Predictor with 3+ Modalities

An hypothesis for a categorical predictor with 3 or more modalities predicts that **at least one group among the 3 groups will have an average significantly different than the other averages**.

### With $location$ the hypothesis is the following

> The average $js\_score$ of employees working in at least one specific $location$ will be significantly different than the average $js\_score$ of employees working in the other $location$.

### In mathematical terms

- $H_0$: it is true that $\mu_{Ireland} = \mu_{France} = \mu_{Australia}$
- $H_a$: it is **not** true that $\mu_{Ireland} = \mu_{France} = \mu_{Australia}$

This analysis is usually preformed using a one-way ANOVA but as ANOVA are special cases of the General Linear Model, let's keep this approach.

---

# Categorical Predictor with 3+ Modalities

```{r}
dnd %>% 
  ggplot(aes(x = location, y =  js_score)) + 
  geom_jitter(width = 0.1) +
  geom_hline(yintercept = mean(dnd$js_score), linetype = "dashed") +
  stat_summary(fun = mean, geom = "errorbar", aes(ymax = ..y.., ymin = ..y..), lwd = 2, color = "blue") +
  theme(
    legend.position = "none",
    text = element_text(size = 20)
  )
```

---

# Categorical Predictor with 3+ Modalities

### In JAMOVI

1. Open your file
2. Set variables according their type
3. Analyses > Regression > Linear Regression
4. Set $js\_score$ as DV and $location$ as Factors
5. In **Model Coefficients** option: select **Omnibus Test ANOVA test**

```{r out.width = "40%"}
knitr::include_graphics("img/jamovi_lm_main_c32.png")
```

### Results

> The is a significant effect of employee's $location$ on their average $js\_score$ ( `r lm_c3$statistic$location`)

---
class: title-slide, middle

## Exercise (10 min)

Using the dnd.csv dataset, test the following models:

1. $$js\_score = \beta_{0} + \beta_{1}.salary\_c + \beta_{2}.location + \epsilon$$

2. $$js\_score = \beta_{0} + \beta_{1}.salary\_c + \beta_{2}.location + \beta_{3}.salary\_c*location + \epsilon$$

3. $$js\_score = \beta_{0} + \beta_{1}.salary + \beta_{2}.location + \epsilon$$

4. $$js\_score = \beta_{0} + \beta_{1}.salary + \beta_{2}.location + \beta_{3}.salary*location + \epsilon$$

5. $$js\_score = \beta_{0} + \beta_{1}.salary + \beta_{2}.location + \beta_{3}.perf + \epsilon$$

6. $$js\_score = \beta_{0} + \beta_{1}.salary + \beta_{2}.location + \beta_{3}.perf + \beta_{4}.salary*location +$$
$$\beta_{5}.perf*location + \beta_{6}.perf*salary + \beta_{7}.salary*location*perf + \epsilon$$

---
class: inverse, mline, left, middle

# 3. Manipulating Contrast with Categorical Predictors

---

# Categorical Coding in Linear Regression

Among the possibilities of recoding a Categorical variable to Continuous, there are some classics called "Dummy Coding" and "Deviation Coding".

---

# Dummy Coding in Linear Regression

Dummy Coding is when a modality is coded 0 and the other coded 1. For example:

```
# In JAMOVI, recode female as 0 and male as 1 (Dummy Coding)
IF(gender == "female", 0, 1)
```

Dummy Coding is useful because one of the modality becomes the intercept and is tested against 0.

.pull-left[

```{r fig.height=4}
df2  %>% 
  dplyr::mutate(gender_c = case_when(
    gender == "Female" ~ 0,
    gender == "Male" ~ 1
  )) %>% 
  ggplot(aes(x = gender_c, y = math_results)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  scale_x_continuous(breaks = c(0, 1), limits = c(-0.5, 1.5)) +
  theme_bw() +
  theme(
    text = element_text(size = 14)
  )
```
]

.pull-right[

```{r results='asis'}
df2 %>% 
  dplyr::mutate(gender_c = case_when(
    gender == "Female" ~ 0,
    gender == "Male" ~ 1
  )) %>% 
  lm(data = ., formula = math_results ~ gender_c) %>% 
  broom::tidy() %>% 
  dplyr::mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) %>% 
  knitr::kable(digits = 2) %>%
  kable_styling(font_size = 12)
```
]

---

# Deviation Coding in Linear Regression

Deviation Coding is when the intercept is situated between the codes of the modalities. For example:

```
# In JAMOVI, recode female as -1 and male as 1 (Deviation Coding)
IF(gender == "female", -1, 1)
```

Deviation Coding is useful because the average of the modalities becomes the intercept and is tested against 0.

However, in the Deviation Coding using -1 vs. +1, the distance between the modalities is 2 not 1. Therefore, even if the test of the slop is the exact same, the value of the slop (the estimate) is twice lower.

Consequently it is possible to use a Deviation Coding with -0.5 vs. +0.5 to keep the distance of 1 between the modalities.

```
# In JAMOVI, recode female as -0.5 and male as 0.5 (Deviation Coding)
IF(gender == "female", -0.5, 0.5)
```

---

# Deviation Coding in Linear Regression

.pull-left[

Female = -1 and Male = 1

```{r fig.height=4}
df2 %>%
  dplyr::mutate(gender_c = case_when(
    gender == "Female" ~ -1,
    gender == "Male" ~ 1
  )) %>% 
  ggplot(aes(x = gender_c, y = math_results)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  scale_x_continuous(breaks = c(-1, 1), limits = c(-1.5, 1.5)) +
  theme_bw() +
  theme(
    text = element_text(size = 14)
  )
```

```{r results='asis'}
df2 %>% 
  dplyr::mutate(gender_c = case_when(
    gender == "Female" ~ -1,
    gender == "Male" ~ 1
  )) %>% 
  lm(data = ., formula = math_results ~ gender_c) %>% 
  broom::tidy() %>% 
  dplyr::mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) %>% 
  knitr::kable(digits = 2) %>%
  kable_styling(font_size = 12)
```
]

.pull-right[

Female = -0.5 and Male = 0.5

```{r fig.height=4}
df2  %>% 
  dplyr::mutate(gender_c = case_when(
    gender == "Female" ~ -0.5,
    gender == "Male" ~ 0.5
  )) %>% 
  ggplot(aes(x = gender_c, y = math_results)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  scale_x_continuous(breaks = c(-0.5, 0.5), limits = c(-1, 1)) +
  theme_bw() +
  theme(
    text = element_text(size = 14)
  )
```

```{r results='asis'}
df2 %>% 
  dplyr::mutate(gender_c = case_when(
    gender == "Female" ~ -0.5,
    gender == "Male" ~ 0.5
  )) %>% 
  lm(data = ., formula = math_results ~ gender_c) %>% 
  broom::tidy() %>% 
  dplyr::mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) %>% 
  knitr::kable(digits = 2) %>%
  kable_styling(font_size = 12)
```
]

---

# Coding Predictors with 3+ Modalities

Y/Outcome/DV = $math\_results$(Continuous variable from 0 to 100)

X/Predictor/IV = $country$ (Categorical variable with 3 modalities: *Ireland*, *France* and *Spain*)

.pull-left[
```{r}
df4 <- data.frame(
  participant = c("ppt1", "ppt2", "ppt3", "ppt4", "ppt5", "ppt6", "ppt7", "ppt8"), 
  country = c("Ireland", "France", "Spain", "Spain", "Ireland", "France", "Spain", "Ireland"),
  math_results = c(89, 64, 71, 77, 96, 69, 59, 99)
) %>% 
  dplyr::mutate(country_c = case_when(
    country == "Ireland" ~ 1,
    country == "France" ~ 2,
    country == "Spain" ~ 3
  ))

knitr::kable(df4, format = "html") %>% 
  kableExtra::kable_styling(font_size = 14)
```
]

.pull-right[
```{r fig.height=5}
df4 %>% 
  ggplot(aes(x = factor(country, level = c("Ireland", "France", "Spain")), y = math_results)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  scale_x_discrete("country") +
  theme_bw() +
  theme(
    text = element_text(size = 14)
  )
```
]

---

# Coding Predictors with 3+ Modalities

$t$-test can only compare 2 modalities. Because Linear Regression Models are (kind of) $t$-test, modalities will be compared 2-by-2 with one modality as the reference to compare all the others.

For example a linear regression of `country` on `math_results` will display not one effect for the `country` but the effect of the 2-by-2 comparison using a reference group by alphabetical order:

```{r results='asis'}
df4 %>% 
  lm(data = ., formula = math_results ~ country) %>% 
  broom::tidy() %>% 
  dplyr::mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) %>% 
  knitr::kable(digits = 2) %>%
  kable_styling(font_size = 12)
```

In our case the reference is the group "France" (first letter).

Here is our problem: **How to test the overall effect of a variable with 3 or more Categories?**

---

# ANOVA Test for Overall Effects

Beside (Multiple) Linear Regression and $t$-test, researchers are using ANOVA a lot. ANOVA, stands for Analysis of Variance and is also a sub category of Linear Regression Models.

ANOVA is used to calculate the overall effect of categorical variable having more that 2 categories as $t$-test cannot cope. In the case of testing 1 categorical variable, a "one-way" ANOVA is performed.

**How ANOVA is working?**

### In real words
- $H_a$: at least one group is different from the others
- $H_0$: all the groups are the same

### In mathematical terms
- $H_a$: it is **not true** that $\mu_{1} = \mu_{2} = \mu_{3}$
- $H_0$: it is **true** that $\mu_{1} = \mu_{2} = \mu_{3}$

---

# ANOVA Test for Overall Effects

I won't go too much in the details but to check if at least one group is different from the others, the distance of each value to the overall mean (Between‚àígroup variation) is compared to the distance of each value to their group mean(Within‚àígroup variation).

If the Between‚àígroup variation is the same as the Within‚àígroup variation, all the groups are the same.

```{r out.width = '100%'}
knitr::include_graphics("img/one_way_anova_basics.png")
```

---

# ANOVA Test for Overall Effects

In JAMOVI, an ANOVA table can be obtain alongside the coefficient table to obtain a summary of the Categorical effect:

```{r}
res <- jmv::linReg(
    data = df4,
    dep = math_results,
    factors = country,
    blocks = list(
        list(
            "country")),
    refLevels = list(
        list(
            var="country",
            ref="France")),
    anova = TRUE)

res$models[[1]][[1]]
```

**ANOVA is only testing if all the group averages are the same or not, but it does not tell us which average is different and how.**

In our example, the ANOVA table told us that there is a significant effect of the variable `country` on `math_results`.

However, the ANOVA test does not tell which group is different from the others. If your hypothesis states a specific difference for one or multiple groups there are two different ways to test it: **Post-hoc tests** or **Contrasts**.

---

# Post-hoc Tests 

The "Post-hoc" runs a separate $t$-test for all the pairwise modality comparison:

```{r}
res <- jmv::ANOVA(
    formula = math_results ~ country,
    data = df4,
    postHoc = ~ country)

res$postHoc[[1]] %>% 
  knitr::kable(digits = 2)
```

Even if it looks useful, "Post-hoc" test can be considered as $p$-Hacking because **there is no specific hypothesis testing, everything is compared**.

Some corrections for multiple tests are available such as Tukey, Scheffe, Bonferroni or Holm, but they are still very close to the bad science boundary.

---

# Contrasts or Factorial ANOVA

Actually, you are mastering Contrasts already. 

With Contrasts you can test specific comparisons with modalities of a Categorical Predictor.

Contrasts is the name for the coding used to convert a categorical variable to continuous like Dummy Coding or Deviation Coding but there are many more type of Coding systems such as Treatment, Sum to zero, Polynomial ...

Unfortunately Contrast have a couple of rules: 
- **modalities with the same code are averaged together**
- **the number of contrast is the number of categories - 1**
- **the value 0 means the modality is not taken into account** (except treatment contrast)

The best way to understand the contrast is by manually creating new variable corresponding to your contrast.

---

# Sum to Zero Contrasts

Also called "Simple" contrast, each contrast encodes the difference between one of the groups and a baseline category, which in this case corresponds to the first group:

```{r}
tibble::tribble(
  ~Modality, ~Contrast1, ~Contrast2,
  "Placebo",  -1, -1,
  "Vaccine 1",  1,  0,
  "Vaccine 2",    0,  1
) %>% 
  knitr::kable()
```

In this example:
- Contrast 1 compares Placebo with Vaccine 1
- Contrast 2 compares Placebo with Vaccine 2

However I won't be able to compare Vaccine 1 and Vaccine 2

---

# Polynomial Contrasts

Used to test linear and even non linear effect, they are the most powerful of all the contrasts: Contrast 1 is called Linear, Contrast 2 is Quadratic, Contrast 3 is Cubic, Contrast 4 is Quartic ...

```{r}
tibble::tribble(
  ~Modality, ~Contrast_1, ~Contrast_2,
  "Low",  -1,  1,
  "Medium",  0, -2,
  "High",    1,  1
) %>% 
  knitr::kable()
```

In this example:
- Contrast 1 checks the linear increase between **Low**, **Medium**, **High**
- Contrast 2 checks the quadratic change between **Low**, **Medium**, **High** 

If the hypothesis specified a linear increase, we would expect Contrast 1 to be significant but Contrast 2 to be non-significant

---

# Comparison of Contrasts Results

```{r}
df5 <- df4 %>% 
  dplyr::mutate(
    treatment_c1 = case_when( country == "Ireland" ~ 1, country == "France" ~ 0, country == "Spain" ~ 0),
    treatment_c2 = case_when( country == "Ireland" ~ 0, country == "France" ~ 0, country == "Spain" ~ 1),
    sum_c1 = case_when( country == "Ireland" ~ 1, country == "France" ~ -1, country == "Spain" ~ 0),
    sum_c2 = case_when( country == "Ireland" ~ 0, country == "France" ~ -1, country == "Spain" ~ 1),
    poly_c1 = case_when( country == "Ireland" ~ 0, country == "France" ~ -1, country == "Spain" ~ 1),
    poly_c2 = case_when( country == "Ireland" ~ -2, country == "France" ~ 1, country == "Spain" ~ 1)
    )
```

Let's see what happens with different contrast to compare the average $js\_score$ according employee's $location$: **France**, **Ireland**, **Spain**

### Sum to Zero Contrasts

.pull-left[
```{r}
tibble::tribble(
  ~Modality, ~sum_c1, ~sum_c2,
  "France",  -1,  -1,
  "Ireland",  1,   0,
  "Spain",    0,   1
) %>% 
  knitr::kable() %>%
  kable_styling(font_size = 17)
```
]

.pull-right[
```{r results='asis'}
df5 %>% 
  lm(data = ., formula = math_results ~ sum_c1 + sum_c2) %>% 
  broom::tidy() %>% 
  dplyr::mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) %>% 
  knitr::kable(digits = 2) %>%
  kable_styling(font_size = 17)
```
]

### Polynomial Contrasts

.pull-left[
```{r}
tibble::tribble(
  ~Modality, ~poly_c1, ~poly_c2,
  "France",  -1,   1,
  "Ireland",  0,  -2,
  "Spain",    1,   1
) %>% 
  knitr::kable() %>%
  kable_styling(font_size = 17)
```
]

.pull-right[
```{r results='asis'}
df5 %>% 
  lm(data = ., formula = math_results ~ poly_c1 + poly_c2) %>% 
  broom::tidy() %>% 
  dplyr::mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) %>% 
  knitr::kable(digits = 2) %>%
  kable_styling(font_size = 17)
```
]

---

# Many more Contrasts

**Deviation**: Compares the mean of each level (except a reference category) to the mean of all of the levels (grand mean)

**Simple**: Like the treatment contrasts, the simple contrast compares the mean of each level to the mean of a specified level. This type of contrast is useful when there is a control group. By default the first category is the reference. However, with a simple contrast the intercept is the grand mean of all the levels of the factors.

**Difference**: Compares the mean of each level (except the first) to the mean of previous levels. (Sometimes called reverse Helmert contrasts)

**Helmert**: Compares the mean of each level of the factor (except the last) to the mean of subsequent levels

**Repeated**: Compares the mean of each level (except the last) to the mean of the subsequent level

**Polynomial**: Compares the linear effect and quadratic effect. The first degree of freedom contains the linear effect across all Categories; the second degree of freedom, the quadratic effect. These contrasts are often used to estimate polynomial trends

---
class: inverse, mline, left, middle

<img class="circle" src="https://github.com/damien-dupre.png" width="250px"/>

# Thanks for your attention and don't hesitate if you have any question!

[`r fa(name = "twitter")` @damien_dupre](http://twitter.com/damien_dupre)  
[`r fa(name = "github")` @damien-dupre](http://github.com/damien-dupre)  
[`r fa(name = "link")` damien-datasci-blog.netlify.app](https://damien-datasci-blog.netlify.app)  
[`r fa(name = "paper-plane")` damien.dupre@dcu.ie](mailto:damien.dupre@dcu.ie)
