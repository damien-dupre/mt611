---
title: "MT611 - Quantitative Research Methods"
subtitle: "Chapter 3: Understanding the General Linear Model"
author: "Damien Dupré"
date: "Dublin City University"
output:
  xaringan::moon_reader:
    css: ["default", "metropolis", "metropolis-fonts", "css/custom_design.css"]
    lib_dir: libs
    nature:
      beforeInit: "libs/cols_macro.js"
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
# libraries --------------------------------------------------------------------
library(tidyverse)
library(knitr)
library(broom)
library(kableExtra)
library(performance)
library(fontawesome)
library(DiagrammeR)
library(patchwork)
library(ggrepel)
library(papaja)
library(ggfortify)
library(ggcorrplot)
library(countdown)
library(anicon)

# general options --------------------------------------------------------------
options(scipen = 999)
set.seed(123)
# chunk options ----------------------------------------------------------------
opts_chunk$set(
  cache.extra = rand_seed, 
  message = FALSE, 
  warning = FALSE, 
  error = FALSE, 
  echo = FALSE,
  cache = FALSE,
  comment = "", 
  fig.align = "center", 
  fig.retina = 3
  )

# data -------------------------------------------------------------------------
iqsize <- read.table(here::here("data/iqsize.txt"), header = TRUE)
organisation_beta <- readr::read_csv(here::here("data/organisation_beta.csv"))  

  # tibble::tibble(
  #   gender = sample(c("male", "female"), 20, replace = TRUE),
  #   location = sample(c("Ireland", "France", "Australia"), 20, replace = TRUE),
  #   perf = rnorm(20, mean = 4, sd = 2),
  #   salary = rnorm(20, mean = 30000, sd = 1000),
  #   js_score = -55 + 0.002 * salary + rnorm(20, mean = 2, sd = 1)
  # ) |>
  # tibble::rownames_to_column("employee") |>
  # dplyr::mutate(
  #   js_score = case_when(
  #     js_score > 10 ~ 10,
  #     js_score < 0 ~ 0,
  #     TRUE ~ js_score
  #   ),
  #   perf = case_when(
  #     perf > 10 ~ 10,
  #     perf < 0 ~ 0,
  #     TRUE ~ perf
  #   ),
  #   salary_c = case_when(
  #     salary >= mean(salary) ~ "high",
  #     salary < mean(salary) ~ "low"
  #   ),
  #   perf_c = case_when(
  #     perf >= mean(perf) + sd(perf) ~ "high",
  #     perf < mean(perf) + sd(perf) & perf >= mean(perf) - sd(perf) ~ "medium",
  #     perf < mean(perf) - sd(perf) ~ "low"
  #   ),
  # ) |>
  # readr::write_csv(here::here("data/organisation_beta.csv"))
  
# analyses ---------------------------------------------------------------------
m_js_high <- mean(organisation_beta$js_score[organisation_beta$salary_c == "high"])
m_js_low <- mean(organisation_beta$js_score[organisation_beta$salary_c == "low"])
lm_1 <- lm(js_score ~ salary, data = organisation_beta) |> apa_print()
lm_2 <- lm(js_score ~ salary*perf, data = organisation_beta) |> apa_print()
lm_c <- lm(js_score ~ salary_c, data = organisation_beta) |> apa_print()
lm_c2 <- lm(js_score ~ salary_c, data = organisation_beta) |> apa_print()
lm_c3 <- lm(js_score ~ location, data = organisation_beta) |> aov() |> apa_print()

```

# Results Section in Academic Papers

The results section is always structured in the same way:

#### 1. Descriptive Statistics

> Short description of every variable involved in the hypotheses (predictors and outcome).

> Made of numeric summaries (Mean and Standard Deviation values), distribution figures, and a correlation matrix.

#### 2. Inferential Statistics

> Indicating the relevance of the model in a conventional style.

> Communicating the test of each hypothesis in a conventional style.

**The results have to indicate for every hypothesis if the null hypothesis is rejected (and the alternative hypothesis considered as plausible) or not rejected.**

---

class: inverse, mline, center, middle

# 1. Regressions for Hypothesis Testing

---

# Vocabulary

"Linear Model", "Linear Regression", "Multiple Regression" or simply "Regression" are all referring to the same model: **The General Linear Model**.

It contains:

- Only one Outcome/Dependent Variable
- One or more Predictor/Independent Variables of any type (categorical or continuous)
- Made of Main and/or Interaction Effects

$$Y = b_{0} + b_{1}\,Predictor\,1 + b_{2}\,Predictor\,2+ ... + b_{n}\,Predictor\,n + e$$

A Linear Regression is used **to test all the hypotheses at once** and to calculate the predictors' estimate.

Specific tests are available for certain type of hypothesis such as T-test or ANOVA but as they are special cases of Linear Regressions, their importance is limited (see [Jonas Kristoffer Lindeløv's blog post: Common statistical tests are linear models](https://lindeloev.github.io/tests-as-linear/)).

---

# General Linear Model Everywhere

.pull-left[
Most of the common statistical models (t-test, correlation, ANOVA; chi-square, etc.) are **special cases of linear models**.

This beautiful simplicity means that there is less to learn. In particular, it all comes down to $y = ax + b$ which most students know from secondary school. 

Unfortunately, **stats intro courses are usually taught as if each test is an independent tool**, needlessly making life more complicated for students and teachers alike.

Here, only **one test is taught to rule them all**: the General Linear Model (GLM).
]

.pull-right[
```{r out.width = "100%"}
include_graphics("https://psyteachr.github.io/msc-data-skills/images/memes/glm_meme.png")
```
]

---

# Applied Example

### Imagine the following case study...

> The CEO of Organisation Beta has problems with the well-being of employees and wants to investigate the relationship between **Job Satisfaction (js_score)**, **salary** and **performance (perf)**.

--

### Therefore the CEO formulate 3 hypotheses:

- $H_{a1}$: $js\_score$ increases when $salary$ increases 
- $H_{a2}$: $js\_score$ increases when $perf$ increases
- $H_{a3}$: The effect of $salary$ on $js\_score$ increases when $perf$ increases

--

### The corresponding model is:

$$js\_score = b_{0} + b_{1}\,salary + b_{2}\,perf + b_{3}\,salary*perf +  e$$

---

# Where the Regression Line comes from?

Draw all the possible lines on the frame. The best line, also called best fit, is the one which has the lowest amount or error. 

.pull-left[
```{r fig.width=5, fig.height=5}
  tibble(
    salary = rnorm(200, mean = 5, sd = 5),
    js_score = -1 + 0.02 * salary + rnorm(200, mean = 2, sd = 0.1),
    b0 = rnorm(200, 0, 1),
    b1 = rnorm(200, 0.02, 0.1)
  ) |> 
  ggplot(aes(salary, js_score)) + 
  geom_abline(aes(intercept = b0, slope = b1), alpha = 1/4) +
  geom_point() +
  theme(
    axis.text.x = element_blank(), 
    axis.text.y = element_blank(),
    text = element_text(size = 20)
  )
```
]
.pull-right[
There are 200 models on this plot, but a lot are really bad! We need to find the good models by making precise our intuition that a good model is "close" to the data. 

Therefore, we need a way to quantify the distance between the data and a model. Then we can fit the model by finding the value of $b_0$ and $b_1$ that generate the model with the smallest distance from this data.
]

---

# Best Model, Lowest Error

For each point this specific prediction error is called **Residual** $e_i$ where $i$ is a specific observation (e.g., employee here).

The error of the model is the sum of the prediction error for each point (distance between actual value and predicted value).

.pull-left[
```{r fig.width=5, fig.height=5}
linear_model_1 <- lm(js_score ~ salary, organisation_beta)

organisation_beta$predicted <- predict(linear_model_1)   # Save the predicted values
organisation_beta$residuals <- residuals(linear_model_1) # Save the residual values

plot_error <- organisation_beta |> 
  ggplot(aes(salary, js_score, label = residuals)) +
  geom_segment(aes(xend = salary, yend = predicted), color = "red") +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw() +
  theme(text = element_text(size = 20))

plotly::ggplotly(plot_error)
```
]
.pull-right[

The line which obtains the lowest error, has the smallest residuals. This line is chosen by the linear regression.

One common way to do this in statistics to use the "Mean-Square Error" (aka $MSE$) or the "Root-Mean-Square Error" (aka $RMSE$). We compute the difference between actual and predicted values, square them, sum them and divide them by $n$ observations (and the take the square root of them for the $RMSE$). 

]

---

# The (Root-)Mean-Square Error

```{r fig.width=12, fig.height=5}
plot_residual <- organisation_beta |> 
  ggplot(aes(salary, js_score)) +
  geom_segment(aes(xend = salary, yend = predicted), colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  scale_y_continuous(limits = c(0, 10)) +
  theme_bw() +
  theme(text = element_text(size = 20))

distance_residual <- organisation_beta |> 
  ggplot(aes(salary, residuals, label = round(residuals, 2))) +
  geom_hline(aes(yintercept = 0), linetype = "dashed") +
  geom_col(colour = "red", fill = "red") +
  geom_text(
    aes(y = residuals, vjust = ifelse(residuals >= 0, 0, 1)),
    position = position_dodge(width = 0.9),
    size = 5
  ) +
  scale_y_continuous(limits = c(-2.5, 2.5)) +
  theme_bw() +
  theme(text = element_text(size = 20))

plot_residual / distance_residual
```

$$MSE = \frac{\sum_{i=1}^{N}(y\,predicted_{i} - y\,actual_{i})^{2}}{N}\;RMSE = \sqrt{\frac{\sum_{i=1}^{N}(y\,predicted_{i} - y\,actual_{i})^{2}}{N}}$$

These calculations has lots of appealing mathematical properties, which we are not going to talk about here. You will just have to take my word for it!

---

# Analysis of the Estimate

Once the best line is found, each estimate of the tested equation is calculated by a software (i.e., $b_0, b_1, ..., b_n$).

- $b_0$ is the intercept and has no interest for hypothesis testing
- $b_1, ..., b_n$ are predictors' effect estimate and each of them is used to test an hypothesis

The predictors' effect estimate $b_1, ..., b_n$ are **the value of the slope of the best line between each predictor** and the outcome. 

It indicates **how many units of the outcome variable increases/decreases/changes when the predictor increases by 1 unit**

Technically, $b$ is a weight or multiplier applied to the Predictor's values to obtain the Outcome's expected values

---

# Analysis of the Estimate

- If $b_1, ..., b_n = 0$, then:
  - The regression line is horizontal (no slope)
  - When the Predictor increases by 1 unit, the Outcome variable does not change
  - **The null alternative hypothesis is not rejected**

--

- If $b_1, ..., b_n > 0$, then:
  - The regression line is positive (slope up)
  - When the Predictor increases by 1 unit, the Outcome variable increases by $b$
  - **The null alternative hypothesis is rejected and the alternative hypothesis considered as plausible**

--

- If $b_1, ..., b_n < 0$, then:
  - The regression line is negative (slope down)
  - When the Predictor increases by 1 unit, the Outcome variable decreases by $b$
  - **The null alternative hypothesis is rejected and the alternative hypothesis considered as plausible**

---

# Significance of Effect's Estimate

The statistical significance of an effect estimate depends on the **strength of the relationship** and on the **sample size**:

- An estimate of $b_1 = 0.02$ can be very small but still significantly different from $b_1 = 0$
- Whereas an estimate of $b_1 = 0.35$ can be stronger but in fact not significantly different from $b_1 = 0$

--

The significance is the probability to obtain your results with your sample in the null hypothesis scenario:

- Also called $p$-value
- Is between 0% and 100% which corresponds to a value between 0.0 and 1.0

**If the $p$-value is lower to 5% or 0.05, then the probability to obtain your results in the null hypothesis scenario is low enough to say that the null hypothesis scenario is rejected and there must be a link between the variables.**

--

Remember that the $p$-value is the probability of the data given the null hypothesis: $P(data|H_0)$.

---

# Estimating Regression's Coefficients

As previously indicated, you will not have to calculate all the possible lines in your data to find the best fit, a software will do it for you:

- JAMOVI, JASP or SPSS have a Graphic User Interface
- R, Python or Julia are language based and have no interface

```{r out.width = "60%"}
include_img("lm_example.png")
```

---

# Estimating Regression's Coefficients

The output of any software is two tables:
- Model Fit Measure Table
- Model Coefficients Table

The **Model Fit Measure** table tests the prediction **accuracy of your overall model** (all predictors taken into account).

The **Model Coefficients** table provide an estimate to each predictor $b_1, ..., b_n$ (as well as the intercept $b_0$). the value of the estimate is statistically tested with a $p$-value to see if it is statistically different from 0 (null hypothesis). Therefore, this table is **used to test each hypotheses** separately.

---

# JAMOVI: Stats. Open. Now.

Jamovi is a statistical spreadsheet software designed to be **easy to use**. Jamovi is a compelling alternative to costly statistical products such as SPSS, SAS and JMP to cite a few.

Jamovi will always be **free and open** because Jamovi is made by the scientific community, for the scientific community.

- It can be **downloaded from its website** https://www.jamovi.org/
- It can also be **used without installation**, in a web browser, https://cloud.jamovi.org/ for **online demo** but this demo undergoes periods of downtime, and may cease functioning (without warning) at any time.

`r faa("exclamation-triangle", animate="flash", speed="slow", color="red")` Book "Learning Statistics with JAMOVI" free here: https://www.learnstatswithjamovi.com/

```{r out.width = "100%"}
include_graphics("https://www.jamovi.org/assets/header-logo.svg")
```

---

# JAMOVI GUI

```{r out.width = "100%"}
include_img("jamovi_gui.png")
```

---

# Anatomy of JAMOVI

### 1. Different symbols for **variable types**

```{r out.width = "15%"}
include_img("jamovi_icons.png")
```

### 2. Distinction between **Factors** and **Covariates**:
  - A Factor is a predictor of type categorical (nominal or ordinal)
  - A Covariate is a predictor of type continuous
  
`r faa("exclamation-triangle", animate="flash", speed="slow", color="red")` Expected variable type is displayed in bottom right corner of boxes

### 3. Customise your analysis by **unfolding optional boxes**

### 4. Two linear regression **tables by default**:
  - Model Fit Measures
  - Model Coefficients

---
class: inverse, mline, left, middle

# 2. Hypothesis with Continuous Predictor

---

# Main Effect Example

.pull-left[
### Variables:
- Outcome = $js\_score$ (from 0 to 10)
- Predictor = $salary$ (from 0 to Inf.)

### Hypothesis:

- $H_a$: $js\_score$ increases when $salary$ increases (i.e., $b_1>0$)
- $H_0$: $js\_score$ stay the same when $salary$ increases (i.e., $b_1=0$)

### Equation:

$$js\_score = b_{0} + b_{1}\,salary + e$$
]

.pull-right[
```{r}
organisation_beta |> 
  dplyr::select(employee, salary, js_score) |> 
  kable(format = "html")
```
]

---

# Main Effect Example

.pull-left[
```{r fig.height=10}
organisation_beta |> 
  ggplot(aes(x = salary, y = js_score, label = employee)) +
  geom_point(color = "red", size = 5) +
  geom_text_repel(point.padding = 0.5, size = 14) +
  scale_y_continuous(limits = c(0, 10)) +
  theme_bw() +
  theme(text = element_text(size = 20))
```
]
.pull-right[
```{r fig.height=10}
organisation_beta |> 
  ggplot(aes(x = salary, y = js_score, label = employee)) +
  geom_point(color = "black", size = 5) +
  geom_smooth(method = "lm", formula = "y ~ x", size = 2, fullrange = TRUE, se = FALSE) +
  geom_hline(yintercept = mean(organisation_beta$js_score), color = "red", size = 2) +
  scale_y_continuous(limits = c(0, 10)) +
  theme_bw() +
  theme(text = element_text(size = 20)) +
  annotate(
    "text", 
    x = 29000, 
    y = 7.5, 
    label = "H[0]:b[1] == 0", 
    color = "red", 
    size = 6,
    parse = TRUE
  ) +
  annotate(
    "text", 
    x = 30500, 
    y = 10, 
    label = "H[1]:b[1] > 0", 
    color = "blue", 
    size = 6,
    parse = TRUE
  )
```
]

---

# Main Effect Example

### In JAMOVI

1. Open your file
2. Set variables as **continuous**
3. **Analyses** > **Regression** > **Linear Regression**
4. Set $js\_score$ as DV (i.e., Outcome) and $salary$ as Covariates (i.e., Continuous Predictor)

```{r out.width = "100%"}
include_img("jamovi_lm_main.png")
```

---
class: title-slide, middle

## Live Demo

---
class: title-slide, middle

## Reporting the **Model Fit Measure Table**
  
---

# Model Fit Measure Table

The **Model Fit Measure** table tests the prediction **accuracy of your overall model** (all predictors taken into account).

$Model_{a}: js\_score = b_{0} + b_{1}\;salary + e\;vs.\; Model_{0}: js\_score = b_{0} + e$

```{r out.width = "25%"}
include_img("jamovi_mfm.png")
```

--

Default Columns:

- The **Model** column indicate the reference of the model in case you want to compare multiple models
- $R$ is the correlation between the outcome variable and all predictors taken into account (i.e., the closer to 1 or -1 the better, however in social science models with more that 0.2 or less than -0.2 are already excellent)
- $R^2$ is the % of variance from the outcome explained by the model (e.g., $R^2 = 0.73$ means the model explains 73% of the variance of the outcome variable). $R^2$ is also called **Coefficient of Determination**

---

# More Model Fit Measures

```{r}
include_img("jamovi_mfm_full.png")
```

--

- $Adjusted\,R^2$ is a more conservative version of $R^2$, usually not reported
- $AIC$, $BIC$ and $RMSE$ are useful to compare multiple models, the lower the better
- **Overall Model F Test** is the statistical test to show that your model have significantly better predictions than a model without any predictor.
  - $F$ is the value of the statistical test comparing the results obtained with this sample using the full model with the results obtained with this sample using a model only with the intercept (i.e., $H_0$)
  - $df1$ is the degree of freedom "between group", its value corresponds to the amount of predictor in your model: $df1 = K$ (this is the easy explanation, see more details page 398 of "Learning Statistics with JAMOVI")
  - $df2$ is the degree of freedom "within group", its value corresponds to the amount of observation minus number of parameters minus 1: $df2 = N - K - 1$.
  - $p$ is the p-value, i.e the probability to obtain our prediction with our sample in the null hypothesis scenario (i.e., $p = P(data|H_0)$)

---

# Communicate Model Fit Measures

To communicate results about a model, APA style is a good guide. **Report the following values** using the Model Fit Measure table with all options:

$$R^2 = value_{R^2}, F(value_{df1},value_{df2}) = value_{F}, p = value_{p}$$

--

Model Fit Measure table with all options from our example and corresponding sentence to include in the result section:

```{r}
include_img("jamovi_mfm_full.png")
```

> The predictions from the alternative model (i.e., with our predictors) are significantly better than the predictions from a null model ( `r lm_1$full_result$modelfit$r2`).

--

Note about p-values:

- If the value in the table is $< 0.001$, then write $p < 0.001$ in the text
- Else write $p = value_p$ (e.g., $p = 0.58$ ) but never $p =< 0.001$

---
class: title-slide, middle

## Reporting the **Model Coefficients Table**

---

# Model Coefficients Table

The **Model Coefficients** table provide an estimate to each predictor $b_1, ..., b_n$ (as well as the intercept $b_0$). the value of the estimate is statistically tested with a $p$-value to see if it is statistically different from 0 (null hypothesis).

```{r}
include_img("jamovi_mc.png")
```

--

Default Columns:

- **Predictor** is the list of variables associated to parameters in your model (main and interaction) which includes the intercept
- **Estimate** is the non-standardized relationship estimate of the best prediction line (expressed in the unit of the variable)
- **SE** is the Standard Error and indicate how spread are the values around the estimate
- $t$ is the value of the statistical test comparing the estimate obtained with this sample with an estimate of 0 (i.e., $H_0$)
- $p$ is the p-value, i.e the probability to obtain our prediction with our sample in the null hypothesis scenario 

---

# More Model Coefficients

```{r}
include_img("jamovi_mc_full.png")
```

--

- **Omnibus ANOVA Test** is an alternative way to test model's coefficient but **use only for a categorical predictor with more than 2 modalities**
- **Estimate Confidence Interval** defines the limits of the range where Estimate are still possible to be in given the sample size
- **Standardize Estimate** indicates the strength and direction of the relationship in term of correlation

--

Note that in our example, because there is only one predictor:

1. The Standardize Estimate is the correlation
2. The F-test in the Model Fit Measure table is the same as the F-test in the Omnibus ANOVA Test
3. The p-value in the Model Fit Measure table is the same as the one in the Omnibus ANOVA Test and in the Model Coefficient table

---

# Communicate Model Coefficients

**Report the following values** using the Model Coefficients table with all options:

$$b = value_b, 95\% CI [value_{lower\,CI}, value_{upper\,CI}], t(df_2) = value_t, p = value_{p}$$

--

Model Coefficients table with all options from our example and corresponding sentence to include in the result section:

```{r}
include_img("jamovi_mc_full.png")
```

> The effect of $salary$ on $js\_score$ is statistically significant, therefore $H_0$ can be rejected (** `r lm_1$full_result$salary`**).

---

# $p$-hacking in Correlation Tables

The use of correlation tables is widespread in the literature, and they are a great tool for descriptive analysis but **do not test your hypotheses with correlation tables**. A good practice is to remove all $p$-values or $p$-stars from them.

$p$-values should only be produced to test an hypothesis that has been already formulated, any other is use is called $p$**-hacking**.

```{r out.width = "40%"}
include_graphics("https://media.makeameme.org/created/what-is-something-5ea39c449c.jpg")
```

---

# Interaction Effect Example

### Variables
- Outcome = $js\_score$ (from 0 to 10)
- Predictor 1 = $salary$ (from 0 to Inf.)
- Predictor 2 = $perf$ (from 0 to 10)

### Hypotheses

--

- $H_{a_{1}}$: $js\_score$ increases when $salary$ increases (i.e., $b_1>0$) 
  - $H_{0_{1}}$: $js\_score$ stay the same when $salary$ increases (i.e., $b_1=0$)

--

- $H_{a_{2}}$: $js\_score$ increases when $perf$ increases (i.e., $b_2>0$) 
  - $H_{0_{2}}$: $js\_score$ stay the same when $perf$ increases (i.e., $b_2=0$)
  
--

- $H_{a_{3}}$: The effect of $salary$ on $js\_score$ increases when $perf$ increases (i.e., $b_3>0$) 
  - $H_{0_{3}}$: The effect of $salary$ on $js\_score$ is the same when $perf$ increases (i.e., $b_3=0$)

---

# Interaction Effect Example

### Equation

$$js\_score = b_{0} + b_{1}\,salary + b_{2}\,perf + b_{3}\,salary*perf + e$$

Note: The test of the interaction $b_{3}\,salary*perf$ corresponds to the test of a new variable for which values of $salary$ and values of $perf$ are multiplied

```{r out.width = "40%"}
include_img("meme_interaction_1.jpg")
```

---

# Interaction Effect Example

### In JAMOVI

1. Open your file
2. Set variables as **continuous**
3. **Analyses** > **Regression** > **Linear Regression**
4. Set $js\_score$ as DV and $salary$ as well as $perf$ as Covariates
4. In **Model Builder** option: 
  - Select both $salary$ and $perf$ to bring them in the covariates at once and to obtain a third term called $salary*perf$

```{r}
include_img("jamovi_lm_int.png")
```

---

# Communicate Results

### Overall model:

> The prediction provided by the model with all predictors is significantly better than a model without predictors ( `r lm_2$full_result$modelfit$r2`).

### Salary Hypothesis:

> The effect of $salary$ on $js\_score$ is statistically significant, therefore $H_{0_{1}}$ can be rejected ( `r lm_2$full_result$salary`).

### Perf Hypothesis:

> The effect of $perf$ on $js\_score$ is not statistically significant, therefore $H_{0_{2}}$ can't be rejected ( `r lm_2$full_result$perf`).

### Interaction Hypothesis:

> The interaction effect is not statistically significant, therefore $H_{0_{3}}$ can't be rejected ( `r lm_2$full_result$salary_perf`).

---

# Representing Interaction Effects

### To create a figure automatically in JAMOVI

- Go to Estimated Marginal Means
- Tick Marginal Means plot
- Select both predictor and bring them in Marginal Mean box at once

```{r out.width = "70%"}
include_img("jamovi_lm_int_plot.png")
```

To plot the interaction between 2 continuous predictors, one of them has to be **transform into categorical ordinal variable** of 3 groups: **+1SD**, **Mean**, **-1SD**.

---

# Representing Interaction Effects

In this case:
- **+1SD** is the group of **high perf** (observations higher than avg. + 1SD)
- **Mean** is the group of **average perf** (observations between avg. + 1SD and avg. - 1SD)
- **-1SD**  is the group of **low perf** (observations lower than avg. - 1SD)

Warning: When representing the results of the linear regression **don't use the QQ-plot instead**

Note: $salary*perf$ is a **two-way interaction** but interactions can three-, four-, n-way such as $salary*perf*gender$ or $salary*perf*gender*age$. However the more complex the interaction, the more difficult to interpret its effect.

---
class: title-slide, middle

## Exercise

Open the file `organisation_beta.csv` in JAMOVI and  ...

1. Reproduce the results obtained by testing the following models:
  - $js\_score = b_{0} + b_{1}\,salary + e$
  - $js\_score = b_{0} + b_{1}\,salary + b_{2}\,perf + b_{3}\,salary*perf + e$
2. Test the following model:
$$js\_score = b_{0} + b_{1}\,gender + b_{2}\,perf + b_{3}\,gender*perf + e$$

```{r}
countdown(minutes = 10, warn_when = 60)
```

---

class: inverse, mline, center, middle

# 3. Hypotheses with Categorical Predictors having 2 Categories

---

# Hypotheses with Categorical Predictors

We will have a deep dive in the processing of Categorical predictor variables with linear regressions:

- How to analyse a Categorical predictor with only 2 categories?
- How to analyse a Categorical predictor with more than 2 categories?

```{r out.width = "50%"}
include_graphics("https://memegenerator.net/img/instances/82194202/what-if-we-enter-a-categorical-variable-in-a-regression-model.jpg")
```

---

# Example of Categorical Coding

.pull-left[
Imagine we sample male and female employees to see if the difference between their job satisfaction averages is due **to sampling luck or is reflecting a real difference in the population**.

That is, **is the difference between male and female employees statistically significant?**
]

.pull-right[
```{r}
organisation_beta |> 
  dplyr::select(employee, gender, js_score) |> 
  kable(format = "html")
```
]

---

# Example of Categorical Coding

.pull-left[

.center[Using a Categorical variable having 2 category, **e.g., comparing female vs. male** ...]

```{r fig.height=5}
organisation_beta |> 
  ggplot(aes(x = gender, y = js_score)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  theme_bw() +
  theme(
    text = element_text(size = 20)
  )
```
]

.pull-right[

.center[... is the same as **comparing female coded 1 and male coded 2**]

```{r fig.height=5}
organisation_beta |> 
  mutate(gender_c = case_when(
    gender == "female" ~ 1,
    gender == "male" ~ 2
  )) |> 
  ggplot(aes(x = gender_c, y = js_score)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  scale_x_continuous(breaks = c(1, 2), limits = c(0.5, 2.5)) +
  theme_bw() +
  theme(
    text = element_text(size = 20)
  )
```
]

By default, Categorical variables **are coded using the alphabetical order** (e.g., here Female first then Male) using 1, 2, 3 and so on.

However, you can recode the variable yourself with your own order by creating a new variable using IF statement (e.g., `IF(gender == "female", 2, 1)` in Jamovi)

---

# Categorical Coding in Linear Regression

.pull-left[

.center[Default: **Female = 1** and **Male = 2**]

```{r fig.height=4}
organisation_beta <- organisation_beta |> 
  mutate(gender_c = case_when(
    gender == "female" ~ 1,
    gender == "male" ~ 2
  ))

organisation_beta |> 
  ggplot(aes(x = gender_c, y = js_score)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  scale_x_continuous(breaks = c(1, 2), limits = c(0.5, 2.5)) +
  theme_bw() +
  theme(
    text = element_text(size = 20)
  )
```

```{r results='asis'}
lm(formula = js_score ~ gender_c, data = organisation_beta) |> 
  tidy() |> 
  mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) |> 
  kable(digits = 2) |>
  kable_styling(font_size = 16)
```
]

.pull-right[

.center[Manual: **Male = 1** and **Female = 2**]

```{r fig.height=4}
organisation_beta <- organisation_beta |> 
  mutate(gender_c = case_when(
    gender == "female" ~ 2,
    gender == "male" ~ 1
  )) 

organisation_beta |> 
  ggplot(aes(x = gender_c, y = js_score)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  scale_x_continuous(breaks = c(1, 2), limits = c(0.5, 2.5)) +
  theme_bw() +
  theme(
    text = element_text(size = 20)
  )
```

```{r results='asis'}
lm(data = organisation_beta, formula = js_score ~ gender_c) |> 
  tidy() |> 
  mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) |> 
  kable(digits = 2) |>
  kable_styling(font_size = 16)
```
]

---

# Categorical Predictor with 2 Categories

Let's use another example with the `organisation_beta.csv` file

### Variable transformation

Instead of using $salary$ as a **continuous variable**, let's convert it as $salary\_c$ which is a **categorical variable**:
- Everything higher than or equal to salary average is labelled "**high**" salary
- Everything lower than salary average is labelled "**low**" salary

### Hypothesis

The $js\_score$ of employees having a **high** $salary\_c$ is different than the $js\_score$ of employees having a **low** $salary\_c$

### In mathematical terms

$$H_a: \mu(js\_score)_{high\,salary} \neq \mu(js\_score)_{low\,salary}$$
$$H_0: \mu(js\_score)_{high\,salary} = \mu(js\_score)_{low\,salary}$$

---

# Categorical Predictor with 2 Categories

An hypothesis of differences between two groups is easily tested with a Linear Regression:

- If $\mu_{1} \neq \mu_{2}$, the slope of the line between these averages is not null (i.e., $b_{1} \neq 0$)
- If $\mu_{1} = \mu_{2}$, the slope of the line between these averages is null (i.e., $b_{1} = 0$ )

### Explanation

.pull-left[
**Comparing the difference between two averages is the same as comparing the slope of the line crossing these two averages**
- If two averages are **not equal**, then **the slope of the line crossing these two averages is not 0**
- If two averages are **equal**, then the **slope of the line crossing these two averages is 0**
]

.pull-right[
```{r fig.width=4, fig.height=4}
organisation_beta |> 
  ggplot(aes(x = salary_c, y = js_score)) + 
  geom_jitter(width = 0.1) +
  geom_segment(x = 1, xend = 2, y = m_js_high, yend = m_js_low, lwd = 2, color = "red") +
  geom_hline(yintercept = (m_js_high + m_js_low)/2, linetype = "dashed") +
  stat_summary(fun = mean, geom = "errorbar", aes(ymax = ..y.., ymin = ..y..), lwd = 2, color = "blue") +
  theme(
    legend.position = "none",
    text = element_text(size = 20)
  ) +
  labs(caption = "high coded 1 and low coded 2 (default)")
```
]

---

# Categorical Predictor with 2 Categories

### `r faa("exclamation-triangle", animate="flash", speed="slow", color="red")` Warning

JAMOVI and other software **automatically code categorical variable following alphabetical order** but sometimes you need to change these codes.

.pull-left[
For example, here **low coded with the value 1** and **high coded with the value 2** would make more sense.

The way how categorical variables are coded will influence the sign of the estimate (positive vs. negative)

But **it doesn't change the value of the statistical test** nor the $p$-value obtained
]

.pull-right[
```{r fig.width=4, fig.height=4}
organisation_beta |> 
  mutate(salary_c = factor(salary_c, levels = c("low", "high"))) |> 
  ggplot(aes(x = salary_c, y = js_score)) + 
  geom_jitter(width = 0.1) +
  geom_segment(x = 1, xend = 2, yend = m_js_high, y = m_js_low, lwd = 2, color = "red") +
  geom_hline(yintercept = (m_js_high + m_js_low)/2, linetype = "dashed") +
  stat_summary(fun = mean, geom = "errorbar", aes(ymax = ..y.., ymin = ..y..), lwd = 2, color = "blue") +
  theme(
    legend.position = "none",
    text = element_text(size = 20)
    )
```
]

---

# Categorical Predictor with 2 Categories

### To sum up

**To test the influence of a categorical predictor** variable either nominal or ordinal **having two categories** (e.g., high vs. low, male vs. female, France vs. Ireland), it is possible to **test if the $b$ associated to this predictor is significantly higher, lower, or different from 0**.

### Equation

$$js\_score = b_{0} + b_{1}\,salary\_c + e$$

### Communicating results

Exactly the same template as for Continuous Predictors:

> The predictions provided by the alternative model are significantly better than those provided by null model ( `r lm_c2$full_result$modelfit$r2`).

> The effect of $salary\_c$ on $js\_score$ is statistically significant, therefore $H_0$ can be rejected ( `r lm_c2$full_result$salary_c`).

---
class: title-slide, middle

## Testing Main Effects with Categorical Predictors

---

# Testing Categorical Predictors

### In JAMOVI

1. Open your file
2. Set variables in their **correct type** (continuous, cat. nominal or cat. ordinal)
3. **Analyses > Regression > Linear Regression**
4. Set $js\_score$ as DV (i.e., Outcome) and $salary\_c$ as Factors (i.e., Categorical Predictor)

```{r out.width = "100%"}
include_img("jamovi_lm_main_c2.png")
```

---

# Testing Categorical Predictors

### Model

> The prediction provided by the model with all predictors is significantly better than a model without predictors (** `r lm_c$full_result$modelfit$r2`**).

### Hypothesis with Default Coding (high = 1 vs. low = 2)

> The effect of $salary\_c$ on $js\_score$ is statistically significant, therefore $H_{0}$ can be rejected (** `r lm_c$full_result$salary_c`**).

### Hypothesis with Manual Coding (low = 1 vs. high = 2)

> The effect of $salary\_c$ on $js\_score$ is statistically significant, therefore $H_{0}$ can be rejected (** `r lm_c2$full_result$salary_c`**).

---

# Coding of Categorical Predictors

Choosing 1 and 2 are **just arbitrary numerical values** but any other possibility will produce the same $p$-value

However, choosing codes separated by 1 is handy because it's easily interpretable, the **estimate corresponds to the change from one category to another**:

> The $js\_score$ of "high" $salary\_c$ employees is `r round(m_js_high - m_js_low, 2)` higher than the $js\_score$ of "low" $salary\_c$ employees (when "low" is coded 1 and "high" coded 2).

---

# Coding of Categorical Predictors

### Special case called **Dummy Coding** when a category is coded 0 and the other 1:
- Then the intercept, value of $js\_score$ when salary is 0 corresponds to the category coded 0
- The test of the intercept is the test of the average for the category coded 0 against an average of 0
- Is called simple effect

### Special case called **Deviation Coding** when a category is coded 1 and the other -1:
- Then the intercept, corresponds to the average between the two categories
- The test of the intercept is the test of the average for the variable
- However, the distance between 1 and -1 is 2 units so the estimate is not as easy to interpret, therefore it is possible to choose categories coded 0.5 vs. -0.5 instead

---

# Dummy Coding in Linear Regression

Dummy Coding is when a category is coded 0 and the other coded 1. 

For example, in JAMOVI recode female as 0 and male as 1 (Dummy Coding):

```
IF(gender == "female", 0, 1)
```

Dummy Coding is useful because one of the category becomes the intercept and is tested against 0.

.pull-left[

```{r fig.height=4}
organisation_beta <- organisation_beta |> 
  mutate(gender_c = case_when(
    gender == "female" ~ 0,
    gender == "male" ~ 1
  ))

organisation_beta |> 
  ggplot(aes(x = gender_c, y = js_score)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  scale_x_continuous(breaks = c(0, 1), limits = c(-0.5, 1.5)) +
  theme_bw() +
  theme(
    text = element_text(size = 20)
  )
```
]

.pull-right[

```{r results='asis'}
lm(data = organisation_beta, formula = js_score ~ gender_c) |> 
  broom::tidy() |> 
  dplyr::mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) |> 
  kable(digits = 2) |>
  kable_styling(font_size = 16)
```
]

---

# Deviation Coding in Linear Regression

Deviation Coding is when the intercept is situated between the codes of the categories. 

For example, in JAMOVI recode female as -1 and male as 1 (Deviation Coding):

```
IF(gender == "female", -1, 1)
```

Deviation Coding is useful because **the average of the categories becomes the intercept** and is tested against 0.

However, in the Deviation Coding **using -1 vs. +1, the distance between the categories is 2** not 1. Therefore, even if the test of the slop is the exact same, the value of the slop (the estimate) is twice lower.

Consequently it is possible to use **a Deviation Coding with -0.5 vs. +0.5 to keep the distance of 1** between the categories.

For example, in JAMOVI recode female as -0.5 and male as 0.5 (Deviation Coding):

```
IF(gender == "female", -0.5, 0.5)
```

---

# Deviation Coding in Linear Regression

.pull-left[

.center[Female = -1 and Male = 1]

```{r fig.height=4}
organisation_beta <- organisation_beta |> 
  mutate(gender_c = case_when(
    gender == "female" ~ -1,
    gender == "male" ~ 1
  ))

organisation_beta |> 
  ggplot(aes(x = gender_c, y = js_score)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  scale_x_continuous(breaks = c(-1, 1), limits = c(-1.5, 1.5)) +
  theme_bw() +
  theme(
    text = element_text(size = 20)
  )
```

```{r results='asis'}
lm(data = organisation_beta, formula = js_score ~ gender_c) |> 
  broom::tidy() |> 
  dplyr::mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) |> 
  kable(digits = 2) |>
  kable_styling(font_size = 16)
```
]

.pull-right[

.center[Female = -0.5 and Male = 0.5]

```{r fig.height=4}
organisation_beta <- organisation_beta |> 
  mutate(gender_c = case_when(
    gender == "female" ~ -0.5,
    gender == "male" ~ 0.5
  ))

organisation_beta |> 
  ggplot(aes(x = gender_c, y = js_score)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  scale_x_continuous(breaks = c(-0.5, 0.5), limits = c(-1, 1)) +
  theme_bw() +
  theme(
    text = element_text(size = 20)
  )
```

```{r results='asis'}
lm(data = organisation_beta, formula = js_score ~ gender_c) |> 
  broom::tidy() |> 
  dplyr::mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) |> 
  kable(digits = 2) |>
  kable_styling(font_size = 16)
```
]

---
class: title-slide, middle

## Testing Interaction Effects with Categorical Predictors

---

# Interaction with Categorical Predictors

### In JAMOVI

1. Open your file
2. Set variables according their type
3. **Analyses > Regression > Linear Regression**
4. Set $js\_score$ as DV and $salary\_c$ as well as $gender$ as Factors
4. In the **Model Builder** option: 
  - Select both $salary\_c$ and $gender$ to bring them in the Factors at once

### Model Tested

$$js\_score = b_{0} + b_{1}\,salary\_c + b_{2}\,gender + b_{3}\,salary\_c*gender + e$$

Note: The test of the interaction effect corresponds to the test of a variable resulting from the multiplication between the codes of $salary\_c$ and the codes of $gender$.

---

# Interaction with Categorical Predictors

```{r out.width = "100%"}
include_img("jamovi_lm_main_cint.png")
```

---
class: title-slide, middle

## Live Demo

---
class: title-slide, middle

## Exercise

With the `organisation_beta.csv` data, test the following models and conclude on each effect:

Model 1: $js\_score = b_{0} + b_{1}\,perf + b_{2}\,gender + b_{3}\,perf*gender + e$

Model 2: $js\_score = b_{0} + b_{1}\,perf + b_{2}\,location + b_{3}\,perf*location+ e$

```{r}
countdown(minutes = 10, warn_when = 60)
```

---

class: inverse, mline, center, middle

# 4. Hypotheses with Categorical Predictor having 3+ Categories

---

# Categorical Predictor with 3+ Categories

### Problem with more than 2 groups

I would like to test the effect of the variable $location$ which has 3 categories: "Ireland", "France" and "Australia".

```{r}
include_img("jamovi_lm_main_c31.png")
```

In the Model Coefficient Table, to test the estimate of $location$, there is not 1 result for $location$ but 2!
- Comparison of "Australia" vs. "France"
- Comparison of "Australia" vs. "Ireland"

**Why multiple $p$-value are provided for the same predictor?**

---

# Coding Predictors with 3+ categories

### Variables
- Outcome = $js\_score$ (from 0 to 10)
- Predictor = $location$ (3 categories: *Australia*, *France* and *Ireland*)

.pull-left[
```{r}
organisation_beta |> 
  select(employee, location, js_score) |> 
  kable(format = "html") |> 
  kable_styling(font_size = 14)
```
]

.pull-right[
```{r fig.height=5}
organisation_beta |> 
  ggplot(aes(location, js_score)) +
  geom_point(color = "black", size = 5) +
  stat_summary(fun = "mean", colour = "blue", size = 6, geom = "point") +
  scale_x_discrete("location") +
  theme_bw() +
  theme(
    text = element_text(size = 20)
  )
```
]

---

# Coding Predictors with 3+ categories

$t$-test can only compare 2 categories. Because Linear Regression Models are (kind of) $t$-test, categories will be compared 2-by-2 with one category as the reference to compare all the others.

For example a linear regression of $location$ on $js\_score$ will display not one effect for the $location$ but the effect of the 2-by-2 comparison using a reference group by alphabetical order:

```{r results='asis'}
lm(data = organisation_beta, formula = js_score ~ location) |> 
  tidy() |> 
  mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) |> 
  kable(digits = 2) |>
  kable_styling(font_size = 16)
```

In our case the reference is the group "Australia" (first letter).

Here is our problem: **How to test the overall effect of a variable with 3 or more Categories?**

---

# ANOVA Test for Overall Effects

BesideLinear Regression and $t$-test, researchers are using ANOVA a lot. ANOVA, stands for Analysis of Variance and is also a sub category of Linear Regression Models.

ANOVA is used to calculate the overall effect of categorical variable having more that 2 categories as $t$-test cannot cope. In the case of testing 1 categorical variable, a "one-way" ANOVA is performed.

**How ANOVA is working?**

### In real words
- $H_a$: at least one group is different from the others
- $H_0$: all the groups are the same

### In mathematical terms
- $H_a$: it is **not true** that $\mu_{1} = \mu_{2} = \mu_{3}$
- $H_0$: it is **true** that $\mu_{1} = \mu_{2} = \mu_{3}$

---

# ANOVA Test for Overall Effects

I won't go too much in the details but to check if at least one group is different from the others, the distance of each value to the overall mean (Between−group variation) is compared to the distance of each value to their group mean (Within−group variation).

**If the Between−group variation is the same as the Within−group variation, all the groups are the same.**

```{r out.width = '100%'}
include_img("one_way_anova_basics.png")
```

---

# ANOVA in our Example

An hypothesis for a categorical predictor with 3 or more categories predicts that **at least one group among the 3 groups will have an average significantly different than the other averages**.

### Hypothesis Formulation

> The $js\_score$ of employees working in at least one specific $location$ will be significantly different than the $js\_score$ of employees working in the other $location$.

### In mathematical terms

- $H_0$: it is true that $\mu(js\_score)_{Ireland} = \mu(js\_score)_{France} = \mu(js\_score)_{Australia}$
- $H_a$: it is **not** true that $\mu(js\_score)_{Ireland} = \mu(js\_score)_{France} = \mu(js\_score)_{Australia}$

This analysis is usually preformed using a one-way ANOVA but as ANOVA are special cases of the General Linear Model, let's keep this approach.

---

# ANOVA in our Example

```{r}
organisation_beta |> 
  ggplot(aes(x = location, y =  js_score)) + 
  geom_jitter(width = 0.1) +
  geom_hline(yintercept = mean(organisation_beta$js_score), linetype = "dashed") +
  stat_summary(fun = mean, geom = "errorbar", aes(ymax = ..y.., ymin = ..y..), lwd = 2, color = "blue") +
  theme(
    legend.position = "none",
    text = element_text(size = 20)
  )
```

---

# ANOVA in our Example

### In JAMOVI

1. Open your file
2. Set variables according their type
3. Analyses > Regression > Linear Regression
4. Set $js\_score$ as DV and $location$ as Factors
5. In the **Model Coefficients** option: 
  - Select **Omnibus Test ANOVA test**

```{r out.width = "40%"}
include_img("jamovi_lm_main_c32.png")
```

### Results

> There is no significant effect of employee's $location$ on their average $js\_score$ ( `r lm_c3$statistic$location`)

---
class: title-slide, middle

## Live Demo

---
class: title-slide, middle

## Exercise

Using the `organisation_beta.csv` file, test the following models and conclude on the hypothesis related to each estimate:

Model 1: $js\_score = b_{0} + b_{1}\,salary + b_{2}\,location + b_{3}\,perf + e$

Model 2: $$js\_score = b_{0} + b_{1}\,salary + b_{2}\,location + b_{3}\,perf + b_{4}\,salary*location +$$
$$b_{5}\,perf*location + b_{6}\,perf*salary + b_{7}\,salary*location*perf + e$$

```{r}
countdown(minutes = 5, warn_when = 60)
```

---
class: inverse, mline, left, middle

# 5. Manipulating Contrast with Categorical Predictors

---

# Post-hoc Tests 

Imagine you want to test the specific difference between France and Ireland, **how to obtain a test of specific categories when using a categorical variable with 3 or more categories?**

The "Post-hoc" runs a separate $t$-test for all the pairwise category comparison:

```{r}
res <- jmv::ANOVA(
    formula = js_score ~ location,
    data = organisation_beta,
    postHoc = ~ location)

res$postHoc[[1]] |> 
  kable(digits = 2)
```

Even if it looks useful, "Post-hoc" test can be considered as $p$-Hacking because **there is no specific hypothesis testing, everything is compared**.

Some corrections for multiple tests are available such as Tukey, Scheffe, Bonferroni or Holm, but they are still very close to the bad science boundary.

---

# Contrasts or Factorial ANOVA

By using specific codes for the categories (also called **contrasts**), it is possible to test more precise hypotheses.

Actually, you are mastering Contrasts already. When a recoding is done on a variable with 2 categories (like Dummy Coding or Deviation Coding), a contrast is applied. 

When a recoding is used on more than 2 categories, three rules have to be applied:

--

- Rule 1: **Categories with the same code are tested together**

> Coding Ireland 1, France 1 and Australia 2 compares Ireland and France versus Australia

--

- Rule 2: **The number of contrast possible is the number of categories - 1**

> $location$ has 3 categories so 2 contrast comparisons can be performed

--

- Rule 3: **The value 0 means the category is not taken into account**

> Coding Ireland 1, France 0 and Australia 2 compares Ireland versus Australia

--

To understand contrasts, the best is to manually creating them in your spreadsheet.

---

# Sum to Zero Contrasts

Also called "Simple" contrast, each contrast encodes the difference between one of the groups and a baseline category, which in this case corresponds to the first group:

.pull-left[
```{r}
contrast_df <- 
  tribble(
    ~`Predictor's categories`, ~Contrast1, ~Contrast2,
    "Placebo",                 -1,         -1,
    "Vaccine 1",                1,          0,
    "Vaccine 2",                0,          1
  )

contrast_df |> 
  kable()
```
]

.pull-right[
```{r fig.height=4}
contrast_df |> 
  mutate(Outcome = c(4, 5, 9)) |> 
  ggplot(aes(`Predictor's categories`, Outcome)) +
  geom_col() +
  theme_bw() +
  theme(text = element_text(size = 20))
```
]

In this example:
- Contrast 1 compares Placebo with Vaccine 1
- Contrast 2 compares Placebo with Vaccine 2

However I won't be able to compare Vaccine 1 and Vaccine 2

---

# Polynomial Contrasts

They are the most powerful of all the contrasts to test linear and non linear effects: Contrast 1 is called Linear, Contrast 2 is Quadratic, Contrast 3 is Cubic, Contrast 4 is Quartic ...

.pull-left[
```{r}
contrast_df <- 
  tribble(
    ~`Predictor's categories`, ~Contrast_1, ~Contrast_2,
    "Low",                     -1,           1,
    "Medium",                   0,          -2,
    "High",                     1,           1
  )

contrast_df |> 
  kable()
```
]

.pull-right[
```{r fig.height=4}
contrast_df |> 
  mutate(
    Outcome = c(4, 5, 9),
    `Predictor's categories` = factor(`Predictor's categories`, levels = c("Low", "Medium", "High"))
  ) |> 
  ggplot(aes(`Predictor's categories`, Outcome)) +
  geom_col() +
  theme_bw() +
  theme(text = element_text(size = 20))
```
]

In this example:
- Contrast 1 checks the linear increase between **Low**, **Medium**, **High**
- Contrast 2 checks the quadratic change between **Low**, **Medium**, **High** 

If the hypothesis specified a linear increase, we would expect Contrast 1 to be significant but Contrast 2 to be non-significant

---
class: title-slide, middle

## Live Demo

---

# Comparison of Contrasts Results

```{r}
organisation_beta <- organisation_beta |> 
  dplyr::mutate(
    treatment_c1 = case_when(
      location == "Ireland" ~ 1, 
      location == "France" ~ 0, 
      location == "Australia" ~ 0
    ),
    treatment_c2 = case_when(
      location == "Ireland" ~ 0, 
      location == "France" ~ 0, 
      location == "Australia" ~ 1
    ),
    sum_c1 = case_when(
      location == "Ireland" ~ 1, 
      location == "France" ~ -1, 
      location == "Australia" ~ 0
    ),
    sum_c2 = case_when(
      location == "Ireland" ~ 0, 
      location == "France" ~ -1, 
      location == "Australia" ~ 1
    ),
    poly_c1 = case_when(
      location == "Ireland" ~ 0, 
      location == "France" ~ -1, 
      location == "Australia" ~ 1
    ),
    poly_c2 = case_when(
      location == "Ireland" ~ -2, 
      location == "France" ~ 1, 
      location == "Australia" ~ 1
    )
  )
```

Let's see what happens with different contrast to compare the average $js\_score$ according employee's $location$: **France**, **Ireland**, **Australia**

### Sum to Zero Contrasts

.pull-left[
```{r}
tribble(
  ~category, ~sum_c1, ~sum_c2,
  "France",  -1,  -1,
  "Ireland",  1,   0,
  "Australia",    0,   1
) |> 
  kable() |>
  kable_styling(font_size = 17)
```
]

.pull-right[
```{r results='asis'}
lm(data = organisation_beta, formula = js_score ~ sum_c1 + sum_c2) |> 
  tidy() |> 
  mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) |> 
  kable(digits = 2) |>
  kable_styling(font_size = 17)
```
]

### Polynomial Contrasts

.pull-left[
```{r}
tribble(
  ~category, ~poly_c1, ~poly_c2,
  "France",  -1,   1,
  "Ireland",  0,  -2,
  "Australia",    1,   1
) |> 
  kable() |>
  kable_styling(font_size = 17)
```
]

.pull-right[
```{r results='asis'}
lm(data = organisation_beta, formula = js_score ~ poly_c1 + poly_c2) |> 
  tidy() |> 
  mutate(p.value = format.pval(round(p.value, 3), eps = 0.001)) |> 
  kable(digits = 2) |>
  kable_styling(font_size = 17)
```
]

---
class: title-slide, middle

## Exercise

1. Using the `organisation_beta.csv` file, create contrast variables to reproduce the results obtained with Sum to Zero Contrasts

2. When it's done, explicit the hypotheses tested, the representation of the models and their corresponding equation

```{r}
countdown(minutes = 10, warn_when = 60)
```

---

# Solution - Sum to Zero Contrasts

Variables:

- Outcome = $js\_score$ (from 0 to 10)
- Predictor 1 = $sum\_c1$ (Ireland vs France)
- Predictor 2 = $sum\_c2$ (Australia vs France)

Hypotheses:

- $H_{a_1}$: The average $js\_score$ of Irish employees is different than the average $js\_score$ of French employees
  - $H_{0_1}$: The average $js\_score$ of Irish employees is the same as the average $js\_score$ of French employees

- $H_{a_2}$: The average $js\_score$ of Australia employees is different than the average $js\_score$ of France employees
  - $H_{0_2}$: The average $js\_score$ of Australia employees is the same as the average $js\_score$ of France employees

---

# Solution - Sum to Zero Contrasts

Model:

```{r eval=TRUE, fig.align="center"}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
    node [shape = circle]
    js_score
    node [shape = square]
    sum_c1, sum_c2
    
    sum_c1 -> js_score [label= b1]
    sum_c2 -> js_score [label= b2]
  }", height = 200, width = 500)
```

Equation:

- $js\_score = b_{0} + b_{1}\,sum\_c1 + b_{2}\,sum\_c2 + e$

---

class: inverse, mline, center, middle

# 6. Assumptions of Regression Model

---

# Assumptions of Regression Model

Statistical tests are widely used to test hypotheses, exactly how we just did but all statistical tests have requirements to meet before being applied.

The General Linear Model has 4 requirements:

## 1. **L**inearity (of the effects)

## 2. **I**ndependence (of observations)

## 3. **N**ormality (of the residuals)

## 4. **E**qual Variance (of the residuals) 

While the assumption of a Linear Model are never perfectly met in reality, we must check if there are reasonable enough assumption that we can work with them.

---

# Assumptions of Regression Model

All assumptions can be checked with Jamovi, except the Independence of observations which is more about self-assessment.

.pull-left[
In the Linear Regression options, open **Assumption Checks** and **tick all the boxes**.

Some boxes will be used for the additional assumptions (following chapter), but 2 are used to check the main assumptions: 
- **Q-Q plot of residuals** to check the normality of residuals (assumption 3) 
- **Residual plots** to check equal variance of the residuals (assumption 4)
]

.pull-right[
```{r}
include_img("lm_assumption_jamovi.png")
```
]

---
class: title-slide, middle

## 1. Linearity (of the effects)

---

# Assumptions 1: Linearity

A pretty fundamental assumption of the linear regression model is that the relationship between X and Y actually is linear.

To check it in Jamovi, **plot the data and have a look**:

```{r fig.height=4, fig.width=10}
df <- data.frame(
  x = c(0,5,10,6 ,9,13,15,16,20,21,24,26,29,30,32,34,36,38,40,43,44,45, 50,60) - 15,
  y = c(0.00,0.10,0.25,0.15,0.24,0.26,0.30,0.31,0.40,0.41,0.49,0.50,0.56, 0.80,0.90,1.00,1.00,1.00,0.80,0.50,0.40,0.20,0.15,0.00)*10
  )

raw_plot <- df |> 
  ggplot(aes(x, y)) +
  geom_point() +
  theme_bw()

linear_model_1 <- lm(y ~ x, data = df)

df$predicted <- predict(linear_model_1)   # Save the predicted values
df$residuals <- residuals(linear_model_1) # Save the residual values

linear_plot <- df |> 
  ggplot(aes(x, y)) +
  geom_segment(aes(xend = x, yend = predicted)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw()

loess_model <- loess(y ~ x, data = df, span = 5)

df$predicted <- predict(loess_model)   # Save the predicted values
df$residuals <- residuals(loess_model) # Save the residual values

loess_plot <- df |> 
  ggplot(aes(x, y)) +
  geom_segment(aes(xend = x, yend = predicted)) +
  geom_point() +
  geom_smooth(method = "loess", span = 5) +
  theme_bw()

raw_plot + linear_plot + loess_plot
```

If the shape of the data is non linear then even the best linear model will have very big residuals and therefore very high $MSE$ or $RMSE$.

---
class: title-slide, middle

## 2. Independence (of observations)

---

# Assumptions 2: Independence

To check this assumption, you need to know how the data were collected. **Is there a reason why two observations could be artificially related?**

> For example, an experiment investigating marriage satisfaction according the duration of the marriage will be flawed if data are collected from both partners. Indeed the satisfaction from 1 member of the couple should be correlated to the satisfaction of the other couple. 

Make sure your participant does not know each others or then use the so called "linear mixed models".

In general, this is really just a "catch all" assumption, to the effect that "there's nothing else funny going on in the residuals". If there is something weird (e.g., the residuals all depend heavily on some other unmeasured variable) going on, it might screw things up.

---
class: title-slide, middle

## 3. Normality (of the residuals)

---

# Assumptions 3: Normality

Like many of the models in statistics, the General Linear Model **assumes that the residuals are normally distributed**. 

Note that **it's actually okay if the predictor and the outcome variables are non-normal**, as long as the residuals $e$ are normal.

### Three kinds of residuals:

- **Ordinary residuals**: Distance between the prediction and the observed value from your variable (called $e_i$). In case of a multiple linear regression with variables using different scales, the ordinary residuals will have different scales as well.

- **Standardised residuals**: Normalised residuals used for comparison between variables having different scales.
 
- **Studentised residuals**: Normalised residuals for even more standardised comparisons between variables having different scales.

---

# Assumptions 3: Normality

To check it in Jamovi, **use a Q-Q plot**

```{r fig.height=4, fig.width=10}
df <- data.frame(
  normal = rnorm(1000)
  ) |> 
  dplyr::mutate(
    skew_right = ifelse(normal > 0, normal * 2.5, normal),
    skew_left = ifelse(normal < 0, normal * 2.5, normal),
    fat_tails = normal * 2.5
  ) |> 
  tidyr::pivot_longer(everything(), names_to = "type", values_to = "value") |> 
  dplyr::mutate(type = factor(type, levels = c("normal", "skew_right", "skew_left", "fat_tails")))
  
distribution_plot <- df |> 
  ggplot(aes(x = value, y = ..density..)) +
  geom_histogram(colour = "black", fill = "white") +
  facet_grid(.~type) +
  theme_bw()

qq_plot <- df |> 
  ggplot(aes(sample = value)) + 
  stat_qq() + 
  stat_qq_line() +
  facet_grid(.~type) +
  theme_bw()

distribution_plot/qq_plot
```

- If the residuals are normality distributed, then the line formed by the dots follows the diagonal line (plot 1 and 4)
- If the residuals are skewed right/left, then the line formed by the dots deviates from the diagonal (plot 2 and 3)

---
class: title-slide, middle

## 4. Equal Variance (of the residuals)

---

# Assumptions 4: Equal Variance

.pull-left[
Also called Homogeneity or Homoscedasticity, the regression model assumes that each residual $e_i$ is generated from a normal distribution: **the standard deviation of the residual should be the same for all values of the Predictor**. 

In Jamovi, use **Residuals Plot** option providing a scatterplot for each predictor variable, the outcome variable, and the predicted values against residuals.

If the Equal Variance is met **we should see no pattern in the first plot, only a cloud of points**.

With other software, a line is drawn in this residual vs. fitted plot. A flat line would indicate Equal Variance of residuals.
]

.pull-right[
```{r out.width="60%"}
include_img("lm_jamovi_residual.png")
```
]

---
class: title-slide, middle

## Live Demo

---
class: title-slide, middle

## Exercise

Test the assumptions of the following linear regression: 

$$js\_score = b_{0} + b_{1}\,perf + e$$

### 1. **L**inearity (of the effects)

### 2. **I**ndependence (of observations)

### 3. **N**ormality (of the residuals)

### 4. **E**qual Variance (of the residuals) 

```{r}
countdown(minutes = 10, warn_when = 60)
```

---
class: inverse, mline, center, middle

# 7. More Assumptions

---

# Uncorrelated Predictors

In a multiple regression model, you don't want your predictors to be too strongly correlated with each other:

* This isn't technically an assumption of the regression model, but in practice it's required
* Predictors that are too strongly correlated with each other (referred to as collinearity) can cause problems when evaluating the model

### How to check it
- JAMOVI: **Regression > Correlation Matrix > Plot Correlation Matrix**

### Example

.pull-left[
Imagine American scientist trying to predict individual's IQ by using their height, weight and the size of their brain as follow:

$$IQ = b_0 + b_1\,Height + b_2\,Weight + b_3\,Brain + e$$
]

.pull-right[
```{r, fig.height=3, fig.width=3}
iqsize |> 
  cor() |> 
  ggcorrplot(
    hc.order = TRUE,
    type = "lower",
    outline.col = "white",
    lab = TRUE
  )
```
]

---

# Uncorrelated Predictors

**Variance Inflation Factors** (VIFs) is a very good measure of the extent to which a variable is correlated with all the other variables in the model. **A cut off value of 5 is commonly used**.

### How to check it

- JAMOVI: **Regression > Linear Regression: Assumption Checks "Collinearity statistics"**

```{r}
res <- jmv::linReg(
    data = iqsize,
    dep = PIQ,
    covs = vars(Brain, Height, Weight),
    blocks = list(
        list(
            "Brain",
            "Height",
            "Weight")),
    refLevels = list(),
    collin = TRUE)

res$models[[1]]$assump$collin$asDF |>
  kable(digits = 2)
```

---

# No Anomalous Data

Again, not actually a technical assumption of the model (or rather, it's sort of implied by all the others), but there is **an implicit assumption that your regression model isn't being too strongly influenced by one or two anomalous data points** because this raises questions about the adequacy of the model and the trustworthiness of the data in some cases.

### Three kinds of anomalous data

.pull-left[
- Harmless Outlier Observations
- High Leverage Observations
- High Influence Observations
]

.pull-right[
```{r}
include_graphics("https://memegenerator.net/img/instances/53809899/outliers-outliers-everywhere.jpg")
```
]

---

# Harmless Outlier Observations

An "harmless" outliers is **an observation that is very different from what the regression model predicts**. In practice, we operationalise this concept by saying that an outlier is an observation that has a very large Studentised residual.

.pull-left[
Outliers are interesting: 

* A big outlier might correspond to junk data, e.g., the variables might have been recorded incorrectly in the data set, or some other defect may be detectable. 
* You shouldn't throw an observation away just because it's an outlier. But the fact that it's an outlier is often a cue to look more closely at that case and try to find out why it's so different.
]

.pull-right[
```{r fig.height=5, fig.width=5}
mydata <- within(data.frame(x=1:10), y <- rnorm(x, mean=x))
fm.orig <- lm(y ~ x, data=mydata)
mydata$y[6] <- 20
fm.lm <- update(fm.orig)
plot(y ~ x, data=mydata)
abline(fm.orig, lty="dashed")    # use a dashed line
abline(fm.lm)
legend(
  "topright", 
  inset=0.03, 
  bty="n",
  legend = c("Fit without outlier", "Fit with outlier"),
  lty = c("dashed", "solid")
  )
```
]


---

# High Leverage Observations

The second way in which an observation can be unusual is if it has high leverage, which happens when the observation is **very different from all the other observations and influences the slope of the linear regression**. 

.pull-left[
This doesn't necessarily have to correspond to a large residual. 

If the observation happens to be unusual on all variables in precisely the same way, it can actually lie very close to the regression line.

High leverage points are also worth looking at in more detail, but they're much less likely to be a cause for concern.
]

.pull-right[
```{r fig.height=5, fig.width=5}
mydata <- within(data.frame(x=1:10), y <- rnorm(x, mean = x))
fm.orig <- lm(y ~ x, data=mydata)
mydata$y[9] <- 5
fm.lm <- update(fm.orig)
plot(y ~ x, data=mydata)
abline(fm.orig, lty="dashed")    # use a dashed line
abline(fm.lm)
legend(
  "topright", 
  inset = 0.03, 
  bty = "n",
  legend = c("Fit without high leverage", "Fit with high leverage"),
  lty = c("dashed", "solid")
  )
```
]

---

# High Influence Observations

A high influence observation is an outlier that has high leverage. That is, it is **an observation that is
very different to all the other ones in some respect, and also lies a long way from the regression line**.

.pull-left[
We operationalise influence in terms of a measure known as **Cook's distance**. 

In Jamovi, information about Cook's distance can be calculated by clicking on the Cook's Distance' checkbox in the **Assumption Checks > Data Summary** options. 

For an observation, a Cook's distance greater than 1 is considered large. However, Jamovi provides only a summary, **check the maximum Cook distance and its average** to know if some observations have high influence.
]

.pull-right[
```{r fig.height=5, fig.width=5}
mydata <- within(data.frame(x=1:10), y <- rnorm(x, mean=x))
fm.orig <- lm(y ~ x, data=mydata)
mydata$y[9] <- 20
fm.lm <- update(fm.orig)
plot(y ~ x, data=mydata)
abline(fm.orig, lty="dashed")    # use a dashed line
abline(fm.lm)
legend(
  "topright", 
  inset = 0.03, 
  bty = "n",
  legend = c("Fit without high influence", "Fit with high influence"),
  lty = c("dashed", "solid")
  )
```
]

---
class: title-slide, middle

## Live Demo

---
class: title-slide, middle

## Exercise

Check if the the relationship between $js\_score$ and $pref$ have some:

### 1. Harmless Outlier Observations
### 2. High Leverage Observations
### 3. High Influence Observations

```{r}
countdown(minutes = 5, warn_when = 60)
```

---
class: inverse, mline, center, middle

# 8. Model Selection

---

# First Warning

Model Selection is also called Hierarchical Linear Regression but is **NOT** Hierarchical Linear Model.

- **Hierarchical Linear Regression** compares 2 or more models with fixed effects
- **Hierarchical Linear Model** compares 2 or more models with random effects (also called Multilevel Model)

Here we are using the term **Model Selection** for **Hierarchical Linear Regression**.

---

# Default Model Testing in Multiple Regression

Imagine you are **testing the model** that includes the variables $X$ and $Z$ have an effect on the variable $Y$ such as:

$$H_a: Y = b_{0} + b_{1}\,X + b_{2}\,Z + e$$

If nothing is specified, the null hypothesis $H_0$ is always the following:

$$H_0: Y = b_{0} + e$$

But when there are multiple predictors, the $p$-values provided are only in reference to this simpliest model.

If you want to evaluate the effect of the variable $Z$ while $X$ is taken into account, it is possible to specify $H_0$ as being not that simple such as:

$$H_0: Y = b_{0} + b_{1}\,X + e$$

This is a Model Comparison!

---

# Default Model Testing in Multiple Regression

Example, imagine a model predicting $js\_score$ with $salary$ and $perf$:

$$js\_score = b_{0} + b_{1}\,salary + b_{2}\,perf + e\;(full\;model)$$

In the Model Fit table of Jamovi, this model will be compared to a null model as follow:

$$js\_score = b_{0} + e\;(null\;model)$$

However it is possible to use a more complicated model to be compared with:

$$js\_score = b_{0} + b_{1}\,salary + e\;(simple\;model)$$

Comparing the full model with a simple model consist in evaluating the added value of a new variable in the model, here $perf$.

---

# Model Comparison

A model comparison can:

- Compare Full Model with a more complicated model (called Simple Model)
- Indicates if a variable is useful in a model

This principle is often referred to as Ockham's razor and is often summarised in terms of the following pithy saying: do not multiply entities beyond necessity. In this context, it means don't chuck in a bunch of largely irrelevant predictors just to boost your $R^2$.

To evaluate the good-fitness of a model, the **Akaike Information Criterion** also called $AIC$ (Akaike 1974) is compared between the models: 

- The smaller the $AIC$ value, the better the model performance
- $AIC$ can be added to the Model Fit Measures output Table when the $AIC$ checkbox is clicked

---

# Model Comparison in JAMOVI

```{r}
res <- jmv::linReg(
  data = organisation_beta,
  dep = js_score,
  covs = vars(
    salary,
    perf
  ),
  blocks = list(
    list("salary"),
    list("perf")
  ),
  refLevels = list(),
  aic = TRUE,
  modelTest = TRUE)
```

In Model Builder create Block 1 as your Simple Model and a New Block 2 with the additional predictor.

Details of Model 1 (Simple Model) and Model 2 (Full Model):

```{r}
res$modelFit$asDF |>
  kable()
```

Evaluation of significant difference between the two models:

```{r}
res$modelComp$asDF |>
  kable()
```

Here the difference between the two models is not statistically significant, therefore adding $perf$ in the full model doesn't help to increase the prediction (as indicated by the $AIC$).

---
class: title-slide, middle

## Live Demo

---
class: title-slide, middle

## Exercise

Compare the following full model:

$$js\_score = b_{0} + b_{1}\,salary + b_{2}\,perf + b_{3}\,salary*perf + e$$

With:

$$js\_score = b_{0} + b_{1}\,salary + b_{2}\,perf + e$$

**Is the addition of the Interaction effect improving the accuracy of the model?**

```{r}
countdown(minutes = 5, warn_when = 60)
```

---
class: inverse, mline, center, middle

# 9. Power Analysis for Sample Size and Effect Size Estimations

---

# Theoretical Principle

Remember, the null hypothesis $H_0$ is the hypothesis that there is no relationship between the Predictor and the Outcome. If the null hypothesis is rejected, we accept the alternative hypothesis $H_a$ of a relationship between the Predictor and the Outcome.

This means that four possible situations can occur when we run hypothesis tests:

- We reject $H_0$ ...
  - and in fact $H_a$ is true. This is a good outcome and one which is most often the motivation for the hypothesis test in the first place (True Positive)
  - but in fact $H_a$ is false. This is known as a **Type I error** (False Positive)

- We fail to reject $H_0$ ...
  - and in fact $H_a$ is false. This is a good outcome (True Negative)
  - but in fact $H_a$ is true. This is known as a **Type II error** (False Negative)

> Statistical power refers to the fourth situation and is the probability that we are able to detect the effect that we are looking for.

---

# Theoretical Principle

```{r out.width='80%'}
include_graphics("https://learning.eupati.eu/pluginfile.php/673/mod_book/chapter/388/eupati-types1-2-errors.png")
```

---

# Statistical Power

The ability to identify an effect if it exists (i.e., statistical power) depends at a minimum on three criteria:

- The significance level $\alpha$ to reject $H_0$ (usually $\alpha$ is set at 0.05 or 5%)
- The size $n$ of the sample being used
- The proportion of the variability of the Outcome variable explained by all the Predictors (i.e., full model), known as the **effect size**

The minimum level of statistical power to achieve is usually set at least 0.8 or 80% (i.e., we want at least a 80% probability that the test will return an accurate rejection of $H_0$).

- If all the criteria are known except $n$, then it is possible to approximate the $n$ necessary to obtain at least a 0.8 or 80% power to correctly reject $H_0$ before running the analysis (**prospective power analysis**).

- It is also possible to evaluate which has been achieved once the analysis has been done (**retrospective power analysis**).

---
class: title-slide, middle

## Sample Size Estimation with Prospective Power Analysis

---

# Sample Size Estimation

The main question of most researchers is:

## .center["How many participant is enough to test the formulated hypotheses?"]

Some would obtain an random answer from their colleagues such as "at least 100" or "at least 50 per groups".

However, **there is an actual exact answer** provided by the power analysis:

## .center["It depends how big is the effect size"]

Prospective Power Analysis is reported in the Method section of research papers in order to **describe how the sample size has been estimated thanks to an approximated effect size at the model level** (e.g., small, medium, or large).

---

# Sample Size Estimation

The values of the approximated effect size depend on the type of model tested:

- A model with 1 Main Effect of a Categorical Predictor with 2 categories uses Cohen's $d$
- All other models including Main and Interaction Effect involving Categorical Predictor with 3+ categories or Continuous Predictor uses Cohen's $f$ 

Note: A model with only Main Effects of Continuous Predictors can use a $f$ or $f^2$

Effect Size Rule of Thumb:

```{r}
tribble(
  ~`Effect Size`, ~Small, ~Medium, ~Large,
  "$d$",   .20, .50, .80,
  "$f$",   .10, .25, .40,
  "$f^2$ ", .02, .15, .35
) |> 
  kable()
```

---

# Sample Size Estimation

Power analyses using Cohen's $f$ effect size (i.e., all model expect the ones using a Cohen's $d$) are calculated with an additional parameter: **Numerator df** (degree of freedom)

The degree of freedom of each effect is added to obtain the **Numerator df**:
  - In main effects
    - Continuous Predictors have 1 df
    - Categorical Predictors have k - 1 df (number of categories - 1)
  - In interaction effects, the df of the predictors involved are multiplied
  
Example:

$$js\_score = b_{0} + b_{1}\,salary + b_{2}\,location + b_{3}\,salary*location + e$$

- $b_{1}\,salary$ has 1 df (Continuous Predictor)
- $b_{2}\,location$ has 2 df (3 locations - 1)
- $b_{3}\,salary*location$ has 2 df (1 * 2)

The model's **Numerator df** is 5 (1+2+2)

Note: The number of groups is usually the same as **Numerator df**

---

# Sample Size Estimation

There are multiple possibilities to perform a power analysis: 

- Websites hosted online such as http://powerandsamplesize.com or https://sample-size.net (but none are satisfying)
- Embedded in Statistical software such as SPSS or Jamovi (but none are satisfying)
- Specific software such as G*power (free and the most used power analysis software)
- Packages for coding languages (like {pwr} in R or `statsmodels` in python)

While the last option will be the best solution after being introduced to R, G*power is the most commonly used software and can be downloaded for free here:
https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html

```{r}
include_graphics("https://www.psychologie.hhu.de/fileadmin/_processed_/f/d/csm_GPowerIcon_b6bfb17f0c.png")
```


---

# Sample Size Estimation

G*power uses 3 characteristics to determine the type of power analysis:
1. **Test family** (e.g., t-test for models with 1 predictor either continuous or having two categories or F-test for all other models)
2. **Statistical test** (e.g., mean comparison, ANOVA, multiple linear regression)
3. **Type of power analysis** (e.g., prospective also called "a priori" or retrospective also called "post hoc")

### Whatever your model is, for Sample Size Estimation with Prospective Power Analysis use:
- **F-tests**
- **ANOVA: Fixed effects, special, main effects and interactions**
- **A priori: Compute required sample size - given $\alpha$, power, and effect size**

---

# Applied Example 1

Model Characteristics with $js\_score$ as Continuous Outcome:
- 1 Main Effect of $gender$ (Predictor with 2 categories: male and female employees)

```{r eval=TRUE, fig.align="left"}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
    node [shape = box]
    
    gender -> js_score
  }", height = 100)
```

**Statistical Power**
- Alpha of 0.05 (5%)
- Power of 0.8 (80% probability of accurately rejecting $H_0$)
- Effect size of $d = 0.5$ (medium)

This tells us that we need an absolute minimum of 128 individuals in our sample (64 male and 64 female employees) for an effect size of $d = 0.5$ to return a significant difference at an alpha of 0.05 with 80% probability.

---

# Applied Example 1

```{r out.width='100%'}
include_img("gpower_exemple_1.png")
```

---

# Applied Example 1

```{r out.width='100%'}
include_img("gpower_exemple_1_bis.png")
```

---

# Applied Example 2

Model Characteristics with $js\_score$ as Continuous Outcome:
- 1 Main Effect of $location$ (Predictor with 3 categories: Irish, French, and Australian)

```{r eval=TRUE, fig.align="left"}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
    node [shape = box]
    
    location -> js_score
  }", height = 100)
```

**Statistical Power**
- Alpha of 0.05 (5%)
- Power of 0.8 (80% probability of accurately rejecting $H_0$)
- Effect size of $f = 0.25$ (medium)

This tells us that we need an absolute minimum of 158 individuals in our sample for an effect size of $f = 0.25$ to return a significant difference at an alpha of 0.05 with 80% probability.

---

# Applied Example 2

```{r out.width='100%'}
include_img("gpower_exemple_2.png")
```

---

# Applied Example 3

Model Characteristics with $js\_score$ as Continuous Outcome:
- 1 Main Effect of $location$ (Predictor with 3 categories: Irish, French, and Australian)
- 1 Main Effect of $salary$ (Continuous Predictor)
- 1 Interaction Effect of $location$ and $salary$

```{r}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
  
    node [shape = box]
    location; salary; js_score
    node [shape = point, width = 0, height = 0]
    ''
    
    location -> ''
    salary -> '' [arrowhead = none]
    ''-> js_score
    
    subgraph {
      rank = same; location; '';
    }
  }", height = 100)
```

**Statistical Power**
- Alpha of 0.05 (5%)
- Power of 0.8 (80% probability of accurately rejecting $H_0$)
- Effect size of $f = 0.25$ (medium)

This tells us that we need an absolute minimum of 211 individuals in our sample for an effect size of $f = 0.25$ to return a significant difference at an alpha of 0.05 with 80% probability.

---

# Applied Example 3

```{r out.width='100%'}
include_img("gpower_exemple_3.png")
```

---
class: title-slide, middle

## Effect Size Estimation with Retrospective Power Analysis

---

# Effect Size Reporting

Reporting effect size for each estimate (each hypothesis) in the Result section is the norm

With the General Linear Model, the effect size of each estimate is the corresponding standardised estimate $\beta$. Therefore, no other information needs to be reported.

However, you will see a lot of different metrics in research papers using different statistical tests for their hypotheses. Here is a list of the most used:
- Cohen's $d$ for a Main Effect in a model with 1 Categorical Predictor with 2 categories
- Eta-squared $\eta^2$ or partial Eta-squared $\eta^2_p$ for all the other models

Notes:
- The $\eta^2_p$ for a particular estimate corresponds to the effect size when the other effects in the model are deliberately ignored.
- Both $\omega^2$ and $\epsilon^2$ (and their partial counterparts, $\omega^2_p$ and $\epsilon^2_p$) are unbiased estimators of the population’s $\eta^2$ (or $\eta^2_p$, respectively), which is especially important is small samples.
- $\eta^2_p$ aims at estimating the effect size in a design where all the variables have been experimentally manipulated (e.g., experimental groups). However, some predictors can only be observed. For such cases, we can use generalized Eta squared $\eta^2_G$.

---
class: inverse, mline, center, middle

# 10. Resonable Statistics

---

# Bad vs Good Science

## $p$-Hacking

Running statistical tests until finding a significant $p$-value is $p$-Hacking. To avoid it, all the hypotheses should be tested with one unique test and not with one test by hypothesis. Ideally if another statistical test has to be run, it should be done on new data.

## $p$-HARKing

$p$-HARKing (Hypothesizing After the Results are Known) is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses.

## Pre-Registration and Open Science

In order to avoid any kind of $p$-Hacking or $p$-HARKing, new possibilities for research pre-registration are possible with the open science framework (https://osf.io/), some journal are also suggesting the publication of the data treatment code and all the data source to replicate the results

---

# Always more homework

.center[**For next lecture, read Chapter 12 of "Learning Statistics with JAMOVI"**]
.center[https://www.learnstatswithjamovi.com/]

```{r out.width='40%'}
include_graphics("https://blog.jamovi.org/assets/images/lsj_cover.png")
```

---
class: inverse, mline, left, middle

<img class="circle" src="https://github.com/damien-dupre.png" width="250px"/>

# Thanks for your attention and don't hesitate to ask if you have any question!

[`r fa(name = "twitter")` @damien_dupre](http://twitter.com/damien_dupre)  
[`r fa(name = "github")` @damien-dupre](http://github.com/damien-dupre)  
[`r fa(name = "link")` damien-datasci-blog.netlify.app](https://damien-datasci-blog.netlify.app)  
[`r fa(name = "paper-plane")` damien.dupre@dcu.ie](mailto:damien.dupre@dcu.ie)
