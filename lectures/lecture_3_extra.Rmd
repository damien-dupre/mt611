---
title: "MT611 - Quantitative Research Methods"
subtitle: "Lecture 3: Understanding the General Linear Model"
author: "Damien Dupré"
date: "Dublin City University"
output:
  xaringan::moon_reader:
    css: ["default", "metropolis", "metropolis-fonts", "css/custom_design.css"]
    lib_dir: libs
    nature:
      beforeInit: "libs/cols_macro.js"
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
# general options --------------------------------------------------------------
options(scipen = 999)
set.seed(123)
# chunk options ----------------------------------------------------------------
knitr::opts_chunk$set(
  cache.extra = knitr::rand_seed, 
  message = FALSE, 
  warning = FALSE, 
  error = FALSE, 
  echo = FALSE,
  cache = FALSE,
  comment = "", 
  fig.align = "center", 
  fig.retina = 3
  )
# libraries --------------------------------------------------------------------
library(tidyverse)
library(knitr)
library(fontawesome)
library(DiagrammeR)
library(patchwork)
library(ggrepel)
library(papaja)
library(ggfortify)
library(ggcorrplot)

# data -------------------------------------------------------------------------
census <- readr::read_csv(here::here("data/ireland_census_2016.csv")) 
iqsize <- read.table(here::here("data/iqsize.txt"), header = TRUE)
organisation_beta <- readr::read_csv(here::here("data/organisation_beta.csv"))  

  # tibble::tibble(
  #   gender = sample(c("male", "female"), 20, replace = TRUE),
  #   location = sample(c("Ireland", "France", "Australia"), 20, replace = TRUE),
  #   perf = rnorm(20, mean = 4, sd = 2),
  #   salary = rnorm(20, mean = 30000, sd = 1000),
  #   js_score = -55 + 0.002 * salary + rnorm(20, mean = 2, sd = 1)
  # ) %>%
  # tibble::rownames_to_column("employee") %>%
  # dplyr::mutate(
  #   js_score = case_when(
  #     js_score > 10 ~ 10,
  #     js_score < 0 ~ 0,
  #     TRUE ~ js_score
  #   ),
  #   perf = case_when(
  #     perf > 10 ~ 10,
  #     perf < 0 ~ 0,
  #     TRUE ~ perf
  #   ),
  #   salary_c = case_when(
  #     salary >= mean(salary) ~ "high",
  #     salary < mean(salary) ~ "low"
  #   ),
  #   perf_c = case_when(
  #     perf >= mean(perf) + sd(perf) ~ "high",
  #     perf < mean(perf) + sd(perf) & perf >= mean(perf) - sd(perf) ~ "medium",
  #     perf < mean(perf) - sd(perf) ~ "low"
  #   ),
  # ) %>%
  # readr::write_csv(here::here("data/organisation_beta.csv"))
  
# analyses ---------------------------------------------------------------------
m_js_high <- mean(organisation_beta$js_score[organisation_beta$salary_c == "high"])
m_js_low <- mean(organisation_beta$js_score[organisation_beta$salary_c == "low"])
lm_1 <- lm(js_score ~ salary, data = organisation_beta) %>% apa_print
lm_2 <- lm(js_score ~ salary*perf, data = organisation_beta) %>% apa_print
lm_c <- lm(js_score ~ salary_c, data = organisation_beta) %>% apa_print
lm_c2 <- organisation_beta %>% 
  dplyr::mutate(salary_c = factor(salary_c, level = c("low", "high"))) %>% 
  lm(js_score ~ salary_c, data = .) %>% apa_print
lm_c3 <- lm(js_score ~ location, data = organisation_beta) %>% aov %>% apa_print

```

class: inverse, mline, center, middle

# Power Analysis for Sample Size and Effect Size Estimations

---

# Theoretical Principle

Remember, the null hypothesis $H_0$ is the hypothesis that a difference does not exist between the groups in the overall population. If the null hypothesis is rejected, we accept the alternative hypothesis $H_a$ that a difference does exist between the groups in the population.

This means that four possible situations can occur when we run hypothesis tests:

- We reject $H_0$...
  - and in fact $H_a$ is true. This is a good outcome and one which is most often the motivation for the hypothesis test in the first place.
  - but in fact $H_a$ is false. This is known as a **Type I error**.

- We fail to reject $H_0$...
  - and in fact $H_a$ is false. This is a good outcome.
  - but in fact $H_a$ is true. This is known as a **Type II error**.

> Statistical power refers to the fourth situation and is the probability that we are able to detect the effect that we are looking for.

---

# Theoretical Principle

```{r out.width='80%'}
knitr::include_graphics("https://learning.eupati.eu/pluginfile.php/673/mod_book/chapter/388/eupati-types1-2-errors.png")
```

---

# Statistical Power

The ability to identify an effect if it exists (i.e., statistical power) depends at a minimum on three criteria:

- The significance level $\alpha$ to reject $H_0$ (usually $\alpha$ is set at 0.05 or 5%)
- The size $n$ of the sample being used
- The proportion of the variability of the Outcome variable explained by all the Predictors (i.e., full model), known as the **effect size**

The minimum level of statistical power to achieve is usually set at least 0.8 or 80% (i.e., we want at least a 80% probability that the test will return an accurate rejection of $H_0$).

- If all the criteria are known except $n$, then it is possible to approximate the $n$ necessary to obtain at least a 0.8 or 80% power to correctly reject $H_0$ before running the analysis (**prospective power analysis**).

- It is also possible to evaluate which has been achieved once the analysis has been done (**retrospective power analysis**).

---
class: title-slide, middle

## Sample Size Estimation with Prospective Power Analysis

---

# Sample Size Estimation

The main question of most researchers is "how many participant is enough to test the formulated hypotheses?"

While some would obtain an approximate answer from their colleagues such as "at least 100" or "at least 50 per groups", there is an actual exact answer provided by the power analysis:

> "It depends how big is the effect size"

Prospective Power Analysis is reported in the Method section of research papers in order to describe how the sample size has been estimated thanks to an approximated effect size at the model level (e.g., small, medium, or large)

---

# Sample Size Estimation

The values of the approximated effect size depend on the type of model tested:

- A model with 1 Main Effect of a Categorical Predictor with 2 categories uses Cohen's $d$
- All other models including Main and Interaction Effect involving Categorical Predictor with 3+ categories or Continuous Predictor uses Cohen's $f$ 

Note: A model with only Main Effects of Continuous Predictors can use a $f$ or $f^2$

Effect Size Rule of Thumb:
```{r}
tribble(
  ~`Effect Size`, ~Small, ~Medium, ~Large,
  "$d$",   .20, .50, .80,
  "$f$",   .10, .25, .40,
  "$f^2$ ", .02, .15, .35
) |> 
  kable()
```

---

# Sample Size Estimation

Power analyses using Cohen's $f$ effect size (i.e., all model expect the ones using a Cohen's $d$) are calculated with an additional parameter: **Numerator df** (degree of freedom)

The degree of freedom of each effect is added to obtain the **Numerator df**:
  - In main effects
    - Continuous Predictors have 1 df
    - Categorical Predictors have k - 1 df (number of categories - 1)
  - In interaction effects, the df of the predictors involved are multiplied
  
Example:

$$js\_score = b_{0} + b_{1}\,salary + b_{2}\,location + b_{3}\,salary*location + e$$

- $b_{1}\,salary$ has 1 df (Continuous Predictor)
- $b_{2}\,location$ has 2 df (3 locations - 1)
- $b_{3}\,salary*location$ has 2 df (1 * 2)

The model's **Numerator df** is 5 (1+2+2)

Note: The number of groups is usually the same as **Numerator df**

---

# Sample Size Estimation

There are multiple possibilities to perform a power analysis: 

- Websites hosted online such as http://powerandsamplesize.com or https://sample-size.net (but none are satisfying)
- Embedded in Statistical software such as SPSS or Jamovi (but none are satisfying)
- Specific software such as G*power (free and the most used power analysis software)
- Packages for coding languages (like {pwr} in R or `statsmodels` in python)

While the last option will be the best solution after being introduced to R, G*power is the most commonly used software and can be downloaded for free here:
https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html

---

# Sample Size Estimation

G*power uses 3 characteristics to determine the type of power analysis:
1. **Test family** (e.g., t-test for models with 1 predictor either continuous or having two categories or F-test for all other models)
2. **Statistical test** (e.g., mean comparison, ANOVA, multiple linear regression)
3. **Type of power analysis** (e.g., prospective also called "a priori" or retrospective also called "post hoc")

Whatever your model is, for Sample Size Estimation with Prospective Power Analysis use:
- **F-tests**
- **ANOVA: Fixed effects, special, main effects and interactions**
- **A priori: Compute required sample size - given $\alpha$, power, and effect size**

---

# Applied Example 1

Model Characteristics with $js\_score$ as Continuous Outcome:
- 1 Main Effect of $gender$ (Categorical Predictor with 2 categories: male and female employees)

Statistical  Power:
- Alpha of 0.05 (5%)
- Power of 0.8 (80% probability of accurately rejecting $H_0$)
- Effect size of $d = 0.5$ (medium)

This tells us that we need an absolute minimum of 128 individuals in our sample (64 male and 64 female employees) for an effect size of $d = 0.5$ to return a significant difference at an alpha of 0.05 with 80% probability.

---

# Applied Example 1

```{r out.width='100%'}
knitr::include_graphics("img/gpower_exemple_1.png")
```

---

# Applied Example 1

```{r out.width='100%'}
knitr::include_graphics("img/gpower_exemple_1_bis.png")
```

---

# Applied Example 2

Model Characteristics with $js\_score$ as Continuous Outcome:
- 1 Main Effect of $location$ (Categorical Predictor with 3 categories: Irish, French, and Australian employees)

Statistical  Power
- Alpha of 0.05 (5%)
- Power of 0.8 (80% probability of accurately rejecting $H_0$)
- Effect size of $f = 0.25$ (medium)

This tells us that we need an absolute minimum of 158 individuals in our sample for an effect size of $f = 0.25$ to return a significant difference at an alpha of 0.05 with 80% probability.

---

# Applied Example 2

```{r out.width='100%'}
knitr::include_graphics("img/gpower_exemple_2.png")
```

---

# Applied Example 3

Model Characteristics with $js\_score$ as Continuous Outcome:
- 1 Main Effect of $location$ (Categorical Predictor with 3 categories: Irish, French, and Australian employees)
- 1 Main Effect of $salary$ (Continuous Predictor)
- 1 Interaction Effect of $location$ and $salary$

Statistical  Power
- Alpha of 0.05 (5%)
- Power of 0.8 (80% probability of accurately rejecting $H_0$)
- Effect size of $f = 0.25$ (medium)

This tells us that we need an absolute minimum of 211 individuals in our sample for an effect size of $f = 0.25$ to return a significant difference at an alpha of 0.05 with 80% probability.

---

# Applied Example 3

```{r out.width='100%'}
knitr::include_graphics("img/gpower_exemple_3.png")
```

---
class: title-slide, middle

## Effect Size Estimation with Retrospective Power Analysis

---

# Effect Size Reporting

Reporting effect size for each estimate (each hypothesis) in the Result section is the norm

With the General Linear Model, the effect size of each estimate is the corresponding standardised estimate $\beta$. Therefore, no other information needs to be reported.

However, you will see a lot of different metrics in research papers using different statistical tests for their hypotheses. Here is a list of the most used:
- Cohen's $d$ for a Main Effect in a model with 1 Categorical Predictor with 2 categories
- Eta-squared $\eta^2$ or partial Eta-squared $\eta^2_p$ for all the other models

Notes:
- The $\eta^2_p$ for a particular estimate corresponds to the effect size when the other effects in the model are deliberately ignored.
- Both $\omega^2$ and $\epsilon^2$ (and their partial counterparts, $\omega^2_p$ and $\epsilon^2_p$) are unbiased estimators of the population’s $\eta^2$ (or $\eta^2_p$, respectively), which is especially important is small samples.
- $\eta^2_p$ aims at estimating the effect size in a design where all the variables have been experimentally manipulated (e.g., experimental groups). However, some predictors can only be observed. For such cases, we can use generalized Eta squared $\eta^2_G$.

---
class: inverse, mline, left, middle

<img class="circle" src="https://github.com/damien-dupre.png" width="250px"/>

# Thanks for your attention and don't hesitate if you have any question!

[`r fa(name = "twitter")` @damien_dupre](http://twitter.com/damien_dupre)  
[`r fa(name = "github")` @damien-dupre](http://github.com/damien-dupre)  
[`r fa(name = "link")` damien-datasci-blog.netlify.app](https://damien-datasci-blog.netlify.app)  
[`r fa(name = "paper-plane")` damien.dupre@dcu.ie](mailto:damien.dupre@dcu.ie)
