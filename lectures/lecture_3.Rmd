---
title: "MT611 - Quantitative Research Methods"
subtitle: "Lecture 3: Understanding the General Linear Model"
author: "Damien Dupré"
date: "Dublin City University"
output:
  xaringan::moon_reader:
    css: ["default", "metropolis", "metropolis-fonts", "css/custom_design.css"]
    lib_dir: libs
    nature:
      beforeInit: "libs/cols_macro.js"
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
# general options --------------------------------------------------------------
options(scipen = 999)
set.seed(123)
# chunk options ----------------------------------------------------------------
knitr::opts_chunk$set(
  cache.extra = knitr::rand_seed, 
  message = FALSE, 
  warning = FALSE, 
  error = FALSE, 
  echo = FALSE,
  cache = FALSE,
  comment = "", 
  fig.align = "center", 
  fig.retina = 3
  )
# libraries --------------------------------------------------------------------
library(tidyverse)
library(fontawesome)
library(DiagrammeR)
library(patchwork)
library(ggrepel)
library(papaja)
library(ggfortify)
library(ggcorrplot)

# data -------------------------------------------------------------------------
census <- readr::read_csv(here::here("data/ireland_census_2016.csv")) 
iqsize <- read.table(here::here("data/iqsize.txt"), header = TRUE)
dnd <- readr::read_csv(here::here("data/dnd.csv"))  

  # tibble::tibble(
  #   gender = sample(c("male", "female"), 20, replace = TRUE),
  #   location = sample(c("Ireland", "France", "Australia"), 20, replace = TRUE),
  #   perf = rnorm(20, mean = 4, sd = 2),
  #   salary = rnorm(20, mean = 30000, sd = 1000),
  #   js_score = -55 + 0.002 * salary + rnorm(20, mean = 2, sd = 1)
  # ) %>%
  # tibble::rownames_to_column("employee") %>%
  # dplyr::mutate(
  #   js_score = case_when(
  #     js_score > 10 ~ 10,
  #     js_score < 0 ~ 0,
  #     TRUE ~ js_score
  #   ),
  #   perf = case_when(
  #     perf > 10 ~ 10,
  #     perf < 0 ~ 0,
  #     TRUE ~ perf
  #   ),
  #   salary_c = case_when(
  #     salary >= mean(salary) ~ "high",
  #     salary < mean(salary) ~ "low"
  #   ),
  #   perf_c = case_when(
  #     perf >= mean(perf) + sd(perf) ~ "high",
  #     perf < mean(perf) + sd(perf) & perf >= mean(perf) - sd(perf) ~ "medium",
  #     perf < mean(perf) - sd(perf) ~ "low"
  #   ),
  # ) %>%
  # readr::write_csv(here::here("data/dnd.csv"))
  
# analyses ---------------------------------------------------------------------
m_js_high <- mean(dnd$js_score[dnd$salary_c == "high"])
m_js_low <- mean(dnd$js_score[dnd$salary_c == "low"])
lm_1 <- lm(js_score ~ salary, data = dnd) %>% apa_print
lm_2 <- lm(js_score ~ salary*perf, data = dnd) %>% apa_print
lm_c <- lm(js_score ~ salary_c, data = dnd) %>% apa_print
lm_c2 <- dnd %>% 
  dplyr::mutate(salary_c = factor(salary_c, level = c("low", "high"))) %>% 
  lm(js_score ~ salary_c, data = .) %>% apa_print
lm_c3 <- lm(js_score ~ location, data = dnd) %>% aov %>% apa_print

```

class: inverse, mline, center, middle

# 1. Regressions for Hypothesis Testing

---

# Vocabulary

"Linear Model", "Linear Regression", "Multiple Regression" or simply "Regression" are all referring to the same model: **The General Linear Model**.

It contains:

- Only one Outcome/Dependent Variable
- One or more Predictor/Independent Variables of any type (categorical or continuous)
- Made of Main and/or Interaction Effects

$$Y = \beta_{0} + \beta_{1} . X_{1} + \beta_{2} . X_{2} + ... + \beta_{n} . X_{n} + \epsilon$$

where $n$ is an expected effect (main or interaction)

**To test the significance of the effects, only one statistical method will be used for all hypotheses types which are Linear Regressions.**

Specific tests are available for certain type of hypothesis such as T-test or ANOVA but as they are special cases of Linear Regressions, their importance is limited (see [Jonas Kristoffer Lindeløv's blog post: Common statistical tests are linear models](https://lindeloev.github.io/tests-as-linear/)).

---

# Notes on the Equations

### 1. Using Greek or Latin alphabet

$$Y = \beta_{0} + \beta_{1} . X_{1} + \epsilon \; vs. \; Y = b_{0} + b_{1} . X_{1} + e$$

**Latin alphabet** (e.g. $b$) is used for parameters that are **non-standardized** (i.e., expressed in the unit of the predictor) whereas **Greek alphabet** (e.g. $\beta$) is used when parameters are **standardized** (i.e., expressed in term of correlation ranging from 1 to -1).

### 2. Using $i$s or not

$$Y = \beta_{0} + \beta_{1} . X_{1} + \epsilon \; vs. \; Y_{i} = \beta_{0} + \beta_{1} . X_{1_{i}} + \epsilon_{i}$$

$i$s indicate "for all possible observations". It is **more correct** in a mathematical writing but it is **usually not used** and will not be included in this lecture to no add to much complexity. 

### 3. Using hats or no hats

$\hat{Y}$ or $\hat{y}$ are used to indicate a **predicted value**. Indeed, an equation predicts values of the outcome variable therefore, $y$ should be $\hat{y}$ but it is generally used only by mathematicians.

---

# General Linear Model Everywhere

Most of the common statistical models (t-test, correlation, ANOVA; chi-square, etc.) are special cases of linear models.

This beautiful simplicity means that there is less to learn. In particular, it all comes down to $y = ax + b$ which most students know from secondary school. 

Unfortunately, stats intro courses are usually taught as if each test is an independent tool, needlessly making life more complicated for students and teachers alike.

```{r out.width = "35%"}
knitr::include_graphics("https://psyteachr.github.io/msc-data-skills/images/memes/glm_meme.png")
```

---

# Applied Example

### Imagine the following case study...

> The CEO of the D&D company has problems with his employee well-being and wants to investigate the relationship between **Job Satisfaction (js_score)**, **salary** and **performance (perf)**.

--

### Therefore the CEO formulate 3 hypotheses:

- The higher employees' $salary$ is, the higher their $js\_score$
- The higher employees' $perf$ is, the higher their $js\_score$
- The effect of $salary$ on $js\_score$ is bigger for high $perf$ than for low $perf$

--

### The corresponding model is:

$$js\_score = \beta_{0} + \beta_{1}.salary + \beta_{2}.perf + \beta_{3}.salary*perf + \epsilon$$

---

# Where the Regression Line comes from?

Draw all the possible lines on the frame. The best line, also called best fit, is the one which has the lowest amount or error. 

.pull-left[
```{r fig.width=5, fig.height=5}
  tibble(
    salary = rnorm(200, mean = 5, sd = 5),
    js_score = -1 + 0.02 * salary + rnorm(200, mean = 2, sd = 0.1),
    b0 = rnorm(200, 0, 1),
    b1 = rnorm(200, 0.02, 0.1)
  ) %>% 
  ggplot(aes(salary, js_score)) + 
  geom_abline(aes(intercept = b0, slope = b1), alpha = 1/4) +
  geom_point() +
  theme(
    axis.text.x = element_blank(), 
    axis.text.y = element_blank(),
    text = element_text(size = 20)
  )
```
]
.pull-right[
There are 200 models on this plot, but a lot are really bad! We need to find the good models by making precise our intuition that a good model is "close" to the data. 

Therefore, we need a way to quantify the distance between the data and a model. Then we can fit the model by finding the value of $\beta_0$ and $\beta_1$ that generate the model with the smallest distance from this data.
]

---

# Best Model, Lowest Error

The error of the model (i.e., $\epsilon$) is the sum of the prediction error for each point (distance between actual value and predicted value)

For each point this specific prediction error is called **Residual**

.pull-left[
```{r fig.width=5, fig.height=5}
linear_model_1 <- lm(js_score ~ salary, dnd)

dnd$predicted <- predict(linear_model_1)   # Save the predicted values
dnd$residuals <- residuals(linear_model_1) # Save the residual values

plot_error <- dnd %>% 
  ggplot(aes(salary, js_score, label = residuals)) +
  geom_segment(aes(xend = salary, yend = predicted), color = "red") +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw() +
  theme(text = element_text(size = 20))

plotly::ggplotly(plot_error)
```
]
.pull-right[

The line which obtains the lowest error, has the smallest residuals. This line is chosen by the linear regression.

One common way to do this in statistics to use the "Mean-Square Error" (aka $MSE$) or the "Root-Mean-Square Error" (aka $RMSE$). We compute the difference between actual and predicted, square them, average them, and the take the square root for the $RMSE$. 

]

---

# The (Root-)Mean-Square Error

```{r fig.width=12, fig.height=5}
plot_residual <- dnd %>% 
  ggplot(aes(salary, js_score)) +
  geom_segment(aes(xend = salary, yend = predicted), colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  scale_y_continuous(limits = c(0, 10)) +
  theme_bw() +
  theme(text = element_text(size = 20))

distance_residual <- dnd %>% 
  ggplot(aes(salary, residuals, label = round(residuals, 2))) +
  geom_hline(aes(yintercept = 0), linetype = "dashed") +
  geom_col(colour = "red", fill = "red") +
  geom_text(
    aes(y = residuals, vjust = ifelse(residuals >= 0, 0, 1)),
    position = position_dodge(width = 0.9),
    size = 5
  ) +
  scale_y_continuous(limits = c(-2.5, 2.5)) +
  theme_bw() +
  theme(text = element_text(size = 20))

plot_residual / distance_residual
```

$$MSE = \frac{\sum_{i=1}^{N}(y\,predicted_{i} - y\,actual_{i})^{2}}{N}\;RMSE = \sqrt{\frac{\sum_{i=1}^{N}(y\,predicted_{i} - y\,actual_{i})^{2}}{N}}$$

These calculations has lots of appealing mathematical properties, which we’re not going to talk about here. You’ll just have to take my word for it!

---

# Significance of the Estimates

The statistical significance of an estimate depends on the **strength of the relationship** and on the **sample size**:

- A standardized estimate of $\beta_1 = 0.02$ can be very small but still significantly different from $\beta_1 = 0$
- Whereas a standardized estimate of $\beta_1 = 0.35$ can be stronger but in fact not significantly different from $\beta_1 = 0$

--

The significance is the probability to obtain your results with your sample knowing that there is no link between the variables in the real life:

- Also called $p$-value
- Is between 0% and 100% which corresponds to a value between 0.0 and 1.0

**If the $p$-value is lower to 5% or 0.05, then the probability to obtain your results (knowing that there is no link between the variables) is low enough to say that there must be a link between the variables.**

--

Remember that the $p$-value is the probability of the data given the null hypothesis: $P(data|H_0)$.

---

# Estimating Regression's Coefficients

As previously indicated, you will not have to calculate all the possible lines in your data to find the best fit, a software will do it for you:

- JAMOVI, JASP or SPSS have a Graphic User Interface
- R, Python or Julia are language based and have no interface

<centre>

```{r out.width = "60%"}
knitr::include_graphics("img/lm_example.png")
```

</centre>

---

# JAMOVI: Stats. Open. Now.

- Can be downloaded on https://www.jamovi.org/

- Book "Learning Statistics with JAMOVI" free here: https://www.learnstatswithjamovi.com/

```{r out.width = "100%"}
knitr::include_graphics("img/jamovi_gui.png")
```

---

# Anatomy of JAMOVI

### 1. Different symbols for **variable types**

```{r out.width = "15%"}
knitr::include_graphics("img/jamovi_icons.png")
```

### 2. Distinction between **Factors** and **Covariates**:
  - A Factor is a predictor of type categorical (nominal or ordinal)
  - A Covariate is a predictor of type continuous
  
Expected variable type is displayed in bottom right corner of boxes

### 3. Customise your analysis by **unfolding optional boxes**

### 4. Two linear regression **tables by default**:
  - Model Fit Measures
  - Model Coefficients

---
class: inverse, mline, left, middle

# 2. Hypothesis with Continuous Predictor

---

# Main Effect Example

.pull-left[
### Variables:
- DV = $js\_score$ (from 0 to 10)
- IV = $salary$ (from 0 to Inf.)

### Hypothesis (one-tailed):

- $H_a$: When $salary$ increases, $js\_score$ increases as well (i.e., $\beta_1>0$)

- $H_0$: When $salary$ increases, $js\_score$ results stay the same (i.e., $\beta_1=0$)

### Equation:

$$js\_score = \beta_{0} + \beta_{1} * salary + \epsilon$$
]

.pull-right[
```{r}
dnd %>% 
  dplyr::select(employee, salary, js_score) %>% 
  knitr::kable(format = "html")
```
]

---

# Main Effect Example

.pull-left[
```{r fig.height=10}
dnd %>% 
  ggplot(aes(x = salary, y = js_score, label = employee)) +
  geom_point(color = "red", size = 5) +
  geom_text_repel(point.padding = 0.5, size = 14) +
  scale_y_continuous(limits = c(0, 10)) +
  theme_bw() +
  theme(text = element_text(size = 20))
```
]
.pull-right[
```{r fig.height=10}
dnd %>% 
  ggplot(aes(x = salary, y = js_score, label = employee)) +
  geom_point(color = "black", size = 5) +
  geom_smooth(method = "lm", formula = "y ~ x", size = 2, fullrange = TRUE, se = FALSE) +
  geom_hline(yintercept = mean(dnd$js_score), color = "red", size = 2) +
  scale_y_continuous(limits = c(0, 10)) +
  theme_bw() +
  theme(text = element_text(size = 20)) +
  annotate(
    "text", 
    x = 29000, 
    y = 7.5, 
    label = "H[0]:beta[1] == 0", 
    color = "red", 
    size = 6,
    parse = TRUE
  ) +
  annotate(
    "text", 
    x = 30500, 
    y = 10, 
    label = "H[1]:beta[1] > 0", 
    color = "blue", 
    size = 6,
    parse = TRUE
  )
```
]

---

# Main Effect Example

### In JAMOVI

1. Open your file
2. Set variables as continuous
3. **Analyses** > **Regression** > **Linear Regression**
4. Set $js\_score$ as DV and $salary$ as Covariates

```{r out.width = "100%"}
knitr::include_graphics("img/jamovi_lm_main.png")
```
  
---

# Model Fit Measure Table

The **Model Fit Measure** table tests the prediction **accuracy of your overall model** (all predictors taken into account).

$Model_{a}: js\_score = \beta_{0} + \beta_{1} * salary + \epsilon\;vs.\; Model_{0}: js\_score = \beta_{0} + \epsilon$

.pull-left[
- The **Model** column indicate the reference of the model in case you want to compare multiple models
]

.pull-right[
```{r out.width = "40%"}
knitr::include_graphics("img/jamovi_mfm.png")
```
]

- $R$ is the correlation between the outcome variable and all predictors taken into account (i.e., the closer to 1 or -1 the better, however in social science models with more that 0.2 or less than -0.2 are already excellent)
- $R \neq r$: $R$ is for a model that can include multiple predictors whereas $r$ is only for 1 predictor 
- $R^2$ is the % of variance from the outcome explained by the model (e.g., $R^2 = 0.73$ means the model explains 73% of the variance of the outcome variable)
- $R^2$ is also called **Coefficient of Determination**

---

# More Model Fit Measures

```{r}
knitr::include_graphics("img/jamovi_mfm_full.png")
```

- $Adjusted\,R^2$ is a more conservative version of $R^2$, usually not reported
- $AIC$, $BIC$ and $RMSE$ are useful to compare multiple models, the lower the better
- **Overall Model F Test** is the statistical test to show that your model have significantly better predictions than a model without any predictor.
  - $F$ is the value of the statistical test comparing the results obtained with this sample using the full model with the results obtained with this sample using a model only with the intercept (i.e., $H_0$)
  - $df1$ is the degree of freedom "between group", its value corresponds to the amount of predictor in your model: $df1 = K$ (this is the easy explanation, see more details page 398 of "Learning Statistics with JAMOVI")
  - $df2$ is the degree of freedom "within group", its value corresponds to the amount of observation minus number of parameters minus 1: $df2 = N - K - 1$.
  - $p$ is the p-value, i.e the probability to obtain our prediction with our sample knowing that predictors have no effect in the real life (i.e., $p = P(data|H_0)$)

---

# Communicate Model Fit Measures

To communicate results about a model, APA style is a good guide. Report as follow:

$R^2 = value_{R^2}$, $F(value_{df1},value_{df2}) = value_{F}$, $p = value_{p}$

From our example:

> The predictions from the model with our predictors are significantly better than the predictions from a model without predictor ( `r lm_1$full_result$modelfit$r2`).

--

Note about p-values: 
 - If p-value is higher than 0.001, write $p = 0.58$
 - If p-value is lower than 0.001, write $p < 0.001$
 - But never $p = 0.000$

---

# Model Coefficients Table

The **Model Coefficients** table tests the prediction accuracy of each predictor (intercept included. This table is **used to test each hypotheses** separately.

```{r}
knitr::include_graphics("img/jamovi_mc.png")
```

- **Predictor** is the list of variables associated to parameters in your model (main and interaction) which includes the intercept
- **Estimate** is the non-standardized relationship estimate of the best prediction line (expressed in the unit of the variable)
- **SE** is the Standard Error and indicate how spread are the values around the estimate
- $t$ is the value of the statistical test comparing the estimate obtained with this sample with an estimate of 0 (i.e., $H_0$)
- $p$ is the p-value, i.e the probability to obtain our prediction with our sample knowing that predictor have no effect in the real life

---

# More Model Coefficients

```{r}
knitr::include_graphics("img/jamovi_mc_full.png")
```

- **Omnibus ANOVA Test** is an alternative way to test model's coefficient but **use only for a categorical predictor with more than 2 modalities**
- **Estimate Confidence Interval** defines the limits of the range where Estimate are still possible to be in given the sample size
- **Standardize Estimate** indicates the strength and direction of the relationship in term of correlation

--

Note that in our example, because there is only one predictor:

1. The Standardize Estimate is the correlation
2. The F-test in the Model Fit Measure table is the same as the F-test in the Omnibus ANOVA Test
3. The p-value in the Model Fit Measure table is the same as the one in the Omnibus ANOVA Test and in the Model Coefficient table

---

# Communicate Model Coefficients

To report the test of a coefficient, use the following:

$b = value_b$, **95% CI** $[value_{lower\,CI}$, $value_{upper\,CI}]$, $t(N - K - 1) = value_t$, $p = value_{p}$

From our example:

> The effect of $salary$ on $js\_score$ is statistically significant, therefore $H_0$ can be rejected (** `r lm_1$full_result$salary`**).

---

# $p$-hacking in Correlation Tables

The use of correlation tables is widespread in the literature, and they are a great tool for descriptive analysis but **do not test your hypotheses with correlation tables**. A good practice is to remove all $p$-values or $p$-stars from them.

$p$-values should only be produced to test an hypothesis that has been already formulated, any other is use is called $p$**-hacking**.

```{r out.width = "40%"}
knitr::include_graphics("https://media.makeameme.org/created/what-is-something-5ea39c449c.jpg")
```

---

# Interaction Effect Example

### Variables
- DV = $js\_score$ (from 0 to 10)
- IV1 = $salary$ (from 0 to Inf.)
- IV2 = $perf$ (from 0 to 10)

### Hypotheses

- $H_{a_{1}}$: When $salary$ increases, $js\_score$ increases as well (i.e., $\beta_1>0$) 
- $H_{0_{1}}$: When $salary$ increases, $js\_score$ results stay the same (i.e., $\beta_1=0$)

- $H_{a_{2}}$: When $perf$ increases, $js\_score$ increases as well (i.e., $\beta_2>0$) 
- $H_{0_{2}}$: When $perf$ increases, $js\_score$ results stay the same (i.e., $\beta_2=0$)

- $H_{a_{3}}$: The effect of $salary$ on $js\_score$ increases when $perf$ increases (i.e., $\beta_3>0$) 
- $H_{0_{3}}$: The effect of $salary$ on $js\_score$ is the same when $perf$ increases (i.e., $\beta_3=0$)

---

# Interaction Effect Example

### Equation

$$js\_score = \beta_{0} + \beta_{1}.salary + \beta_{2}.perf + \beta_{3}.salary*perf + \epsilon$$

Note: The test of the interaction $\beta_{3}.salary*perf$ corresponds to the test of a new variable for which values of $salary$ and values of $perf$ are multiplied

```{r out.width = "40%"}
knitr::include_graphics("img/meme_interaction_1.jpg")
```

---

# Interaction Effect Example

### In JAMOVI

1. Open your file
2. Set variables as **continuous**
3. Analyses > Regression> Linear Regression
4. Set $js\_score$ as DV and $salary$ as well as $perf$ as Covariates
4. In **Model Builder** option: Select both $salary$ and $perf$ to bring them in the covariates at once and to obtain a third term called $salary*perf$

```{r}
knitr::include_graphics("img/jamovi_lm_int.png")
```

---

# Communicate Results

### Overall model:

> The prediction provided by the model with all predictors is significantly better than a model without predictors ( `r lm_2$full_result$modelfit$r2`).

### Salary Hypothesis:

> The effect of $salary$ on $js\_score$ is statistically significant, therefore $H_{0_{1}}$ can be rejected ( `r lm_2$full_result$salary`).

### Perf Hypothesis:

> The effect of $perf$ on $js\_score$ is not statistically significant, therefore $H_{0_{2}}$ can't be rejected ( `r lm_2$full_result$perf`).

### Interaction Hypothesis:

> The interaction effect is not statistically significant, therefore $H_{0_{3}}$ can't be rejected ( `r lm_2$full_result$salary_perf`).

---

# Representing Interaction Effects

### To create a figure automatically in JAMOVI

- Go to Estimated Marginal Means
- Tick Marginal Means plot
- Select both predictor and bring them in Marginal Mean box at once

```{r out.width = "70%"}
knitr::include_graphics("img/jamovi_lm_int_plot.png")
```

To plot the interaction between 2 continuous predictors, one of them has to be **transform into categorical ordinal variable** of 3 groups: **+1SD**, **Mean**, **-1SD**.

---

# Representing Interaction Effects

In our case:
- +1SD is the group of high perf (observations higher than average + 1SD)
- Mean is the group of average perf (observations between average + 1SD and average - 1SD)
- -1SD  is the group of low perf (observations lower than average - 1SD)

Warning: When representing the results of the linear regression **don't use the QQ-plot instead**

Note: $salary*perf$ is a **two-way interaction** but interactions can three-, four-, n-way such as $salary*perf*gender$ or $salary*perf*gender*age$. However the more complex the interaction, the more difficult to interpret its effect.

---
class: title-slide, middle

## Exercise (10 min)

1. Open the dataset dnd.csv in JAMOVI and **reproduce the results obtained by testing**:
$$js\_score = \beta_{0} + \beta_{1}.salary + \epsilon$$
$$js\_score = \beta_{0} + \beta_{1}.salary + \beta_{2}.perf + \beta_{3}.salary*perf + \epsilon$$

2. Create a new variable $salary\_perf$ by multiplying the values of $salary$ and $perf$ and **test the following model**:
$$js\_score = \beta_{0} + \beta_{1}.salary + \beta_{2}.perf + \beta_{3}.salary\_perf + \epsilon$$

3. **Test a model which contains only**:
$$js\_score = \beta_{0} + \beta_{1}.salary + \beta_{2}.salary*perf + \epsilon$$

---

class: inverse, mline, center, middle

# 3. Assumptions of Regression Model

---

# Assumptions of Regression Model

People are using Linear Regression for all hypotheses testing, exactly how I just did but...

Linear Regression (as well as t-test and ANOVA should be used **only if requirements are meet**:

## 1. Linearity (of the effects). 

## 2. Normality (of the residuals).

## 3. Independence (of observations). 

## 4. Homoscedasticity (of the residuals) 

While the assumption of a Linear Model are never perfectly met in reality, we must check if there are reasonable enough assumption that we can work with them.

---

# Assumptions 1: Linearity

A pretty fundamental assumption of the linear regression model is that the relationship between X and Y actually is linear.

**How to check it**: Plot the data and have a look (here simulated data)

```{r fig.height=4, fig.width=10}
df <- data.frame(
  x = c(0,5,10,6 ,9,13,15,16,20,21,24,26,29,30,32,34,36,38,40,43,44,45, 50,60) - 15,
  y = c(0.00,0.10,0.25,0.15,0.24,0.26,0.30,0.31,0.40,0.41,0.49,0.50,0.56, 0.80,0.90,1.00,1.00,1.00,0.80,0.50,0.40,0.20,0.15,0.00)*10
  )

raw_plot <- df %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  theme_bw()

linear_model_1 <- lm(y ~ x, data = df)

df$predicted <- predict(linear_model_1)   # Save the predicted values
df$residuals <- residuals(linear_model_1) # Save the residual values

linear_plot <- df %>% 
  ggplot(aes(x, y)) +
  geom_segment(aes(xend = x, yend = predicted)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw()

loess_model <- loess(y ~ x, data = df, span = 5)

df$predicted <- predict(loess_model)   # Save the predicted values
df$residuals <- residuals(loess_model) # Save the residual values

loess_plot <- df %>% 
  ggplot(aes(x, y)) +
  geom_segment(aes(xend = x, yend = predicted)) +
  geom_point() +
  geom_smooth(method = "loess", span = 5) +
  theme_bw()


raw_plot + linear_plot + loess_plot

```

If the shape of the data is non linear then even the best linear model will have very big residuals and therefore very high $MSE$ or $RMSE$.

---

# Assumptions 2: Normality

Like many of the models in statistics, simple or multiple linear regression relies on an assumption of normality. Specifically, **it assumes that the residuals are normally distributed**. **It's actually okay if the predictors X and the outcome Y are non-normal**, as long as the residuals $\epsilon$ are normal.

### Three kinds of residuals:

- **Ordinary residuals**: Distance between the prediction and the observed value from your variable (called $\epsilon$). In case of a multiple linear regression with variables using different scales, the ordinary residuals will have different scales as well.

- **Standardised residuals**: Normalised residuals used for comparison between variables having different scales.
 
- **Studentised residuals**: Normalised residuals for even more standardised comparisons between variables having different scales.

---

# Assumptions 2: Normality with QQ-plot

Plot of the theoretical quantiles according to the model, against the quantiles of the standardised residuals (JAMOVI: Assumption Checks > Q-Q plot of residuals).

```{r fig.height=4, fig.width=10}
df <- data.frame(
  normal = rnorm(1000)
  ) %>% 
  dplyr::mutate(
    skew_right = ifelse(normal > 0, normal * 2.5, normal),
    skew_left = ifelse(normal < 0, normal * 2.5, normal),
    fat_tails = normal * 2.5
  ) %>% 
  tidyr::pivot_longer(everything(), names_to = "type", values_to = "value") %>% 
  dplyr::mutate(type = factor(type, levels = c("normal", "skew_right", "skew_left", "fat_tails")))
  
distribution_plot <- df %>% 
  ggplot(aes(x = value, y = ..density..)) +
  geom_histogram(colour = "black", fill = "white") +
  facet_grid(.~type) +
  theme_bw()

qq_plot <- df %>% 
  ggplot(aes(sample = value)) + 
  stat_qq() + 
  stat_qq_line() +
  facet_grid(.~type) +
  theme_bw()

distribution_plot/qq_plot
```

- If the standardised residuals are normality distributed, then the QQ-plot has a straight line (plot 1 and 4)
- If the standardised residuals are skewed right/left, then the QQ-plot is attracted in the opposite direction (plot 2 and 3)

---

# Assumptions 3: Homogeneity

The regression model assumes that each residual $\epsilon$ is generated from a normal distribution: **the standard deviation of the residual should be the same for all values of Y**. 

.pull-left[
In Jamovi, use **Residuals Plot** option providing a scatterplot for each predictor variable, the outcome variable, and the predicted values against residuals.

If the linearity assumption is met **we should see no pattern here, only a cloud of points**.
]

.pull-right[
```{r, fig.height=5, fig.width=5}
model <- lm(fert_r ~ percent_owner_occupier_with_mortgage_households + percent_owner_occupier_no_mortgage_households + percent_private_rented + percent_social_rented + percent_rented_free_of_rent_households, data = census)

test <- autoplot(model, label.size = 3, alpha = 0.1) +
  theme_classic() +
  theme(text = element_text(size = 14))

test@plots[[1]]
```
]

---

# Assumptions 4: Independent Residuals

To check this assumption, you need to know how the data were collected. **Is there a reason why two observations could be artificially related?**

> For example, an experiment investigating marriage satisfaction according the duration of the marriage will be flawed if data are collected from both partners. Indeed the satisfaction from 1 member of the couple should be correlated to the satisfaction of the other couple. 

Make sure your participant does not know each others or then use the so called "linear mixed models".

In general, this is really just a "catch all" assumption, to the effect that "there's nothing else funny going on in the residuals". If there is something weird (e.g., the residuals all depend heavily on some other unmeasured variable) going on, it might screw things up.

---
class: title-slide, middle

## Exercise (10 min)

Test the assumptions of the following linear regression: 

$$js\_score = \beta_{0} + \beta_{1}.salary + \epsilon$$

### 1. Linearity (of the effects). 

### 2. Normality (of the residuals).

### 3. Independence (of observations). 

### 4. Homoscedasticity (of the residuals) 

---
class: inverse, mline, center, middle

# 4. More Assumptions

---

# Uncorrelated Predictors

In a multiple regression model, you don't want your predictors to be too strongly correlated with each other:

* This isn't technically an assumption of the regression model, but in practice it's required
* Predictors that are too strongly correlated with each other (referred to as collinearity) can cause problems when evaluating the model

### How to check it
- JAMOVI: **Regression > Correlation Matrix > Plot Correlation Matrix**

### Example

.pull-left[
Imagine American scientist trying to predict individual's IQ by using their height, weight and the size of their brain as follow:

$$IQ = \beta_0 + \beta_1Height + \beta_2Weight + \beta_3Brain + \epsilon$$
]

.pull-right[
```{r, fig.height=3, fig.width=3}
iqsize %>% 
  cor() %>% 
  ggcorrplot::ggcorrplot(
    hc.order = TRUE,
    type = "lower",
    outline.col = "white",
    lab = TRUE
  )
```
]


---

# Uncorrelated Predictors

**Variance Inflation Factors** (VIFs) is a very good measure of the extent to which a variable is correlated with all the other variables in the model. **A cut off value of 5 is commonly used**.

### How to check it

- JAMOVI: **Regression > Linear Regression: Assumption Checks "Collinearity statistics"**

```{r}
res <- jmv::linReg(
    data = iqsize,
    dep = PIQ,
    covs = vars(Brain, Height, Weight),
    blocks = list(
        list(
            "Brain",
            "Height",
            "Weight")),
    refLevels = list(),
    collin = TRUE)

res$models[[1]]$assump$collin$asDF %>%
  knitr::kable(digits = 2)
```

---

# No Anomalous Data

Again, not actually a technical assumption of the model (or rather, it's sort of implied by all the others), but there is **an implicit assumption that your regression model isn't being too strongly influenced by one or two anomalous data points** because this raises questions about the adequacy of the model and the trustworthiness of the data in some cases.

### Three kinds of anomalous data

- Harmless Outlier Observations
- High Leverage Observations
- High Influence Observations

---

# Harmless Outlier Observations

An "harmless" outliers is **an observation that is very different from what the regression model predicts**. In practice, we operationalise this concept by saying that an outlier is an observation that has a very large Studentised residual.

.pull-left[
Outliers are interesting: 

* A big outlier might correspond to junk data, e.g., the variables might have been recorded incorrectly in the data set, or some other defect may be detectable. 
* You shouldn't throw an observation away just because it's an outlier. But the fact that it's an outlier is often a cue to look more closely at that case and try to find out why it's so different.
]

.pull-right[
```{r fig.height=5, fig.width=5}
mydata <- within(data.frame(x=1:10), y <- rnorm(x, mean=x))
fm.orig <- lm(y ~ x, data=mydata)
mydata$y[6] <- 20
fm.lm <- update(fm.orig)
plot(y ~ x, data=mydata)
abline(fm.orig, lty="dashed")    # use a dashed line
abline(fm.lm)
legend(
  "topright", 
  inset=0.03, 
  bty="n",
  legend = c("Fit without outlier", "Fit with outlier"),
  lty = c("dashed", "solid")
  )
```
]


---

# High Leverage Observations

The second way in which an observation can be unusual is if it has high leverage, which happens when the observation is **very different from all the other observations and influences the slope of the linear regression**. 

.pull-left[
This doesn't necessarily have to correspond to a large residual. 

If the observation happens to be unusual on all variables in precisely the same way, it can actually lie very close to the regression line.

High leverage points are also worth looking at in more detail, but they're much less likely to be a cause for concern.
]

.pull-right[
```{r fig.height=5, fig.width=5}
mydata <- within(data.frame(x=1:10), y <- rnorm(x, mean = x))
fm.orig <- lm(y ~ x, data=mydata)
mydata$y[9] <- 5
fm.lm <- update(fm.orig)
plot(y ~ x, data=mydata)
abline(fm.orig, lty="dashed")    # use a dashed line
abline(fm.lm)
legend(
  "topright", 
  inset = 0.03, 
  bty = "n",
  legend = c("Fit without high leverage", "Fit with high leverage"),
  lty = c("dashed", "solid")
  )
```
]

---

# High Influence Observations

A high influence observation is an outlier that has high leverage. That is, it is **an observation that is
very different to all the other ones in some respect, and also lies a long way from the regression line**.

.pull-left[
We operationalise influence in terms of a measure known as **Cook's distance**. 

In Jamovi, information about Cook's distance can be calculated by clicking on the Cook's Distance' checkbox in the **Assumption Checks > Data Summary** options. 

A Cook's distance greater than 1 is considered large.
]

.pull-right[
```{r fig.height=5, fig.width=5}
mydata <- within(data.frame(x=1:10), y <- rnorm(x, mean=x))
fm.orig <- lm(y ~ x, data=mydata)
mydata$y[9] <- 20
fm.lm <- update(fm.orig)
plot(y ~ x, data=mydata)
abline(fm.orig, lty="dashed")    # use a dashed line
abline(fm.lm)
legend(
  "topright", 
  inset = 0.03, 
  bty = "n",
  legend = c("Fit without high influence", "Fit with high influence"),
  lty = c("dashed", "solid")
  )
```
]

---
class: title-slide, middle

## Exercise (5 min)

Check if the the relationship between $js\_score$ and $salary$ as well as between $js\_score$ and $pref$ have some:

### 1. Harmless Outlier Observations
### 2. High Leverage Observations
### 3. High Influence Observations

---
class: inverse, mline, center, middle

# 5. Model Selection

---

# First Warning

Model Selection is also called Hierarchical Linear Regression but is **NOT** Hierarchical Linear Model.

- **Hierarchical Linear Regression** compares 2 or more models with fixed effects
- **Hierarchical Linear Model** compares 2 or more models with random effects (also called Multilevel Model)

Here we are using the term **Model Selection** for **Hierarchical Linear Regression**.

---

# Default Model Testing in Multiple Regression

Imagine you are **testing the model** that includes the variables $X$ and $Z$ have an effect on the variable $Y$ such as:

$$H_a: Y = \beta_{0} + \beta_{1}.X + \beta_{2}.Z + \epsilon$$

If nothing is specified, the null hypothesis $H_0$ is always the following:

$$H_0: Y = \beta_{0} + \epsilon$$

But when there are multiple predictors, the $p$-values provided are only in reference to this simpliest model.

If you want to evaluate the effect of the variable $Z$ while $X$ is taken into account, it is possible to specify $H_0$ as being not that simple such as:

$$H_0: Y = \beta_{0} + \beta_{1}.X + \epsilon$$

This is a Model Comparison!

---

# Default Model Testing in Multiple Regression

Example, imagine a model predicting $js\_score$ with $salary$ and $perf$:

$$js\_score = \beta_{0} + \beta_{1}.salary + \beta_{2}.perf + \epsilon\;(full\;model)$$

In the Model Fit table of Jamovi, this model will be compared to a null model as follow:

$$js\_score = \beta_{0} + \epsilon\;(null\;model)$$

However it is possible to use a more complicated model to be compared with:

$$js\_score = \beta_{0} + \beta_{1}.salary + \epsilon\;(simple\;model)$$

Comparing the full model with a simple model consist in evaluating the added value of a new variable in the model, here $perf$.

---

# Model Comparison

A model comparison can:

- Compare Full Model with a more complicated model (called Simple Model)
- Indicates if a variable is useful in a model

This principle is often referred to as Ockham's razor and is often summarised in terms of the following pithy saying: do not multiply entities beyond necessity. In this context, it means don't chuck in a bunch of largely irrelevant predictors just to boost your $R^2$.

To evaluate the good-fitness of a model, the **Akaike Information Criterion** also called $AIC$ (Akaike 1974) is compared between the models: 

- The smaller the $AIC$ value, the better the model performance
- $AIC$ can be added to the Model Fit Measures output Table when the $AIC$ checkbox is clicked

---

# Model Comparison in JAMOVI

```{r}
res <- jmv::linReg(
  data = dnd,
  dep = js_score,
  covs = vars(
    salary,
    perf
  ),
  blocks = list(
    list("salary"),
    list("perf")
  ),
  refLevels = list(),
  aic = TRUE,
  modelTest = TRUE)
```

In Model Builder create Block 1 as your Simple Model and a New Block 2 with the additional predictor.

Details of Model 1 (Simple Model) and Model 2 (Full Model):

```{r}
res$modelFit$asDF %>%
  knitr::kable()
```

Evaluation of significant difference between the two models:

```{r}
res$modelComp$asDF %>%
  knitr::kable()
```

Here the difference between the two models is not statistically significant, therefore adding $perf$ in the full model doesn't help to increase the prediction (as indicated by the $AIC$).

---
class: title-slide, middle

## Exercise (5 min)

Compare the following full model:

$$js\_score = \beta_{0} + \beta_{1}.salary + \beta_{2}.perf + \beta_{3}.salary*perf + \epsilon$$

With:

$$js\_score = \beta_{0} + \beta_{1}.salary + \beta_{2}.perf + \epsilon$$
x
**Is the addition of the Interaction effect improving the accuracy of the model?**

---
class: inverse, mline, center, middle

# 6. Resonable Statistics

---

# Bad vs Good Science

## $p$-Hacking

Running statistical tests until finding a significant $p$-value is $p$-Hacking. To avoid it, all the hypotheses should be tested with one unique test and not with one test by hypothesis. Ideally if another statistical test has to be run, it should be done on new data.

## $p$-HARKing

$p$-HARKing (Hypothesizing After the Results are Known) is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses.

## Pre-Registration and Open Science

In order to avoid any kind of $p$-Hacking or $p$-HARKing, new possibilities for research pre-registration are possible with the open science framework (https://osf.io/), some journal are also suggesting the publication of the data treatment code and all the data source to replicate the results

---

# Always more homework

**For next lecture, read Chapter 12 of "Learning Statistics with JAMOVI"**
https://www.learnstatswithjamovi.com/

---
class: inverse, mline, left, middle

<img class="circle" src="https://github.com/damien-dupre.png" width="250px"/>

# Thanks for your attention and don't hesitate if you have any question!

[`r fa(name = "twitter")` @damien_dupre](http://twitter.com/damien_dupre)  
[`r fa(name = "github")` @damien-dupre](http://github.com/damien-dupre)  
[`r fa(name = "link")` damien-datasci-blog.netlify.app](https://damien-datasci-blog.netlify.app)  
[`r fa(name = "paper-plane")` damien.dupre@dcu.ie](mailto:damien.dupre@dcu.ie)
